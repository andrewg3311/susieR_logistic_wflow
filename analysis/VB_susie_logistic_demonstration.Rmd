---
title: "susieR Logistic Regression VB"
author: "Andrew Goldstein"
date: "May 8, 2019"
output:
  workflowr::wflow_html:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, error = T, warning = T, message = F, fig.align = 'center')
```

# Introduction
This page aims to explore an analog to SuSiE applied to 0/1 data using logistic regression. In my first attempt, I tried to replace the Single Effect Regression (SER) step in the Iterative Bayesian Forward Selection (IBFS) algorithm with an analogous logistic regression step. See [here](susie_logistic_VB_demonstration.html). Here, instead of relying on the linear SuSiE results and modifying the updates to what the analogous updates seem like they should be in the logistic case, I start from the ground up and derive the variational updates in this case directly.

Our response is $\mathbf{y} \in \mathbb{R}^n$, and our covariates are $\mathbf{X} \in \mathbb{R}^{n \times p}$.


## Full Model
The full SuSiE model is as follows:
$$
\begin{aligned}
\mathbf{y} \sim \text{Bern}\Bigg(\frac{e^\mathbf{Xb}}{1 + e^{\mathbf{Xb}}}\Bigg) \quad \text{(element-wise)} \\
\mathbf{b} = \sum_{l = 1}^L \mathbf{b}_l \\
\mathbf{b}_l = \gamma_l b_l \quad (\text{independently for } l = 1, \dots, L) \\
\gamma_l \sim \text{Mult}(1, \mathbf{\pi}) \\
b_l \sim \mathcal{N}(0, \sigma_{0l}^2)
\end{aligned}
$$

## Variational Algorithm
As in linear SuSiE, we optimize over the class of distributions that factorize over $b_l$, i.e. $Q(\mathbf{b}) = \prod_{l=1}^L q_l(\mathbf{b}_l)$. Normally, the updates would be $q_l(\mathbf{b}_l) \propto \exp\{\mathbb{E}_{-(q_l)}[\log (p(y|\mathbf{b}_1, \dots, \mathbf{b}_L) \cdot p(\mathbf{b}_1, \dots, \mathbf{b}_L)\}$. However, this gets ugly quickly, in large part due to the $\log(1 + \exp\{X\mathbf{b}\})$ term in the likelihood.

Instead, we can use a lower bound, $h$ on the likelihood $p(y|\mathbf{b}) \ge h(\mathbf{b}; \xi)$ in our algorithm ($\xi$ are variational parameters that we optimize over). So the derivation of VB is
$$
\log(p(y)) = \log \Big(\int p(y|b) p(b) db\Big) \ge \log \Big(\int h(b;\xi) p(b) db\Big) \ge [\text{Jensen}] \ge \int q(b) \log \frac{h(b;\xi)p(b)}{q(b)} db
$$
Proceeding in the same manner as the general derivation of VB, we obtain the following updates:
$$
\begin{aligned}
q_l(\mathbf{b}_l) \propto \exp\{\mathbb{E}_{-(q_l)}[\log (h(\mathbf{b}; \xi) \cdot p(\mathbf{b}_l)\} \\
\xi = \arg \max_\xi \mathbb{E}_{\mathbf{b} \sim Q}[\log h(\mathbf{b};\xi)]
\end{aligned}
$$

For our lower bound, we turn to ["A Variational Approach to Bayesian Logistic Regression Models and their Extensions" (Jaakkola and Jordan, 1996)](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.210). In this paper, they state the following lower bound result in the logistic response setting:

Let $g(x) = \frac{e^x}{1 + e^x}$ (the inverse logit transformation). Then
$$
P(y_i|x_i, \beta) = g(X_y) \ge g(\xi) \exp\Big\{\frac{X_y - \xi}{2} + \lambda(\xi)(X_y^2 - \xi^2)\Big\}
$$
where $X_y = (2y_i - 1)x_i^T \beta$ and $\lambda(\xi) = \frac{1}{2\xi} (g(\xi) - \frac{1}{2})$.
(The same bound is used in ["Sparse Bayesian Logistic Regression with Hierarchical Prior and Variational Inference" (Horii, 2017)](http://approximateinference.org/2017/accepted/Horri2017.pdf).

Using this bound, we derive the VB updates as follows:
$$
\begin{aligned}
q_l(b_l = c \cdot e_j) \propto \exp\Bigg\{\log p(b_l = c e_j) + \sum_{i=1}^n \Big[\log g(\xi_i) + y_i x_i^T(ce_j + \sum_{k \ne l} \bar{b_k} - \\
\frac{x_i^T(ce_j + \sum_{k \ne l} \bar{b_k}) + \xi_i}{2} - \frac{1}{2\xi_i}\Big(g(\xi_i) - \frac{1}{2}\Big)\Big((\mathbb{E}_{-(q_l)}[(x_i^T\sum_{k=1}^L b_k)^2] - \xi_i^2\Big)\Big]\Bigg\} \\
\text{Note: } \mathbb{E}_{-(q_l)}[(x_i^T\sum_{k=1}^L b_k)^2]  = \mathbb{E}_{-(q_l)}[(c x_i^Te_j + \sum_{k \ne l}x_i^Tb_k)^2] = \\
c^2 (x_i^Te_j)^2 + x_i^T \mathbb{E}_{-(q_l)}[(\sum_{k \ne l}b_k)^2] + 2(cx_i^Te_j)(x_i^T\sum_{k \ne l}\bar{b_k}) = c^2 (x_i^Te_j)^2 + 2(cx_i^Te_j)(x_i^T\sum_{k \ne l}\bar{b_k}) + (const \; in \; b_l) \\
\therefore q_l(b_l = c \cdot e_j) \propto \exp\Bigg\{\log p(b_l = c e_j) + \sum_{i=1}^n \Big[\log g(\xi_i) + y_i x_i^T(ce_j + \sum_{k \ne l} \bar{b_k} - \frac{x_i^T(ce_j + \sum_{k \ne l} \bar{b_k}) + \xi_i}{2} - \\
\frac{1}{2\xi_i}\Big(g(\xi_i) - \frac{1}{2}\Big)\Big((c^2(x_i^Te_j)^2 + (cx_i^Te_j)(x_i^T \sum_{k \ne l} \bar{b_k}) - \xi_i^2\Big)\Big]\Bigg\} \propto \\
[\text{collect terms and complete the square}] \\
\propto \exp\Bigg\{\log(\pi_j) - \frac{\tau_{jl}}{2}\Bigg(c - \frac{\nu_{jl}}{\tau_{jl}}\Bigg)^2 + \frac{\nu_{jl}^2}{2\tau_{jl}} \pm \frac12 \log(1 / \tau_{jl})\Bigg\} \\
\text{where } \nu_{jl} = \sum_{i=1}^n x_i^Te_j\Big(y_i - \frac{1}{2} - \frac{1}{\xi_i}(g(\xi_i) - \frac{1}{2})(x_i^T \sum_{k \ne l}\bar{b_k})\Big) \text{ and } \tau_{jl} = \frac{1}{\sigma_0^2} + \sum_{i=1}^n \frac{1}{\xi_i}(g(\xi_i) - \frac{1}{2})(x_i^Te_j)^2
\end{aligned}
$$
Note that this is a mixture of a multinoulli draw for the non-zero element of $b_l$ and then a normal draw for the value of that non-zero element. In particular,
$$
\begin{aligned}
q(\gamma_l = j) \propto \pi_j \cdot \sqrt{\frac{1}{\tau_{jl}}} \cdot \exp \Bigg\{\frac{\nu_{jl}^2}{2\tau_{jl}}\Bigg\} \\
b_{l_j}|\gamma_l = j \sim_{q_l} \mathcal{N}\Big(\frac{\nu_{jl}}{\tau_{jl}}, \frac{1}{\tau_{jl}}\Big)
\end{aligned}
$$

For the updates to $\xi$:
$$
\begin{aligned}
\xi = \arg \max_\xi \mathbb{E}_{b \sim Q}[\log h(\beta; \xi)] = \arg \max_\xi \sum_{i=1}^n \log(g(\xi_i)) + y_i x_i^T\mathbb{E}_{b \sim Q}[\mathbf{b}] - \frac{x_i^T\mathbb{E}_{b \sim Q}[\mathbf{b}] + \xi_i}{2} - \frac{1}{2 \xi_i}(g(\xi_i) - \frac{1}{2})(\mathbb{E}_{b \sim Q}[(x_i^T\mathbf{b})^2] - \xi_i^2) = \\
[\text{remove constants in } \xi_i] = \arg \max_\xi \sum_{i=1}^n \log(g(\xi_i))- \frac{\xi_i}{2} - \frac{1}{2 \xi_i}(g(\xi_i) - \frac{1}{2})(\mathbb{E}_{b \sim Q}[(x_i^T\mathbf{b})^2] - \xi_i^2)
\end{aligned}
$$
Note that this is separable in $\xi_i$, so we can optimize over each $\xi_i$ separately.
Note further that
$$
\begin{aligned}
Q_i := \mathbb{E}_{b \sim Q}[(x_i^T\mathbf{b})^2] = \mathbb{E}_{b_1 \sim q_1, \dots, b_L \sim q_L}[(x_i^T \sum_{l=1}^L b_l)^2] = \mathbb{E}[\sum_{l=1}^L(x_i^T b_l)^2 + 2\sum_{l=1}^L \sum_{k > l}^L (x_i^T b_l)(x_i^T b_k)] = \\
[\text{mean field approximation, so } b_l \perp b_k] = \sum_{l=1}^L \mathbb{E}_{q_l}[(x_i^Tb_l)^2] + 2 \sum_{l=1}^L \sum_{k > l}^L (x_i^T \bar{b_l})(x_i^T \bar{b_k}) \\
\text{where } \mathbb{E}_{q_l}[(x_i^Tb_l)^2] = \sum_{j=1}^p \alpha_{jl} x_{ij}^2(\sigma_{jl}^2 + \mu_{jl}^2)
\end{aligned}
$$
Where $\alpha_{jl}, \sigma_{jl}^2, \mu_{jl}$ are the posterior inclusion probability for entry $j$ in $b_l$, the posterior variance of that entry when selected, and the posterior mean of that entry when selected, selectively.

According to Wolfram Alpha, by taking the derivative of the objective w.r.t. $\xi_i$, we get a derivative of
$$
\frac{(-2e^{\xi_i} \xi_i + e^{2\xi_i} - 1) \cdot (\xi_i^2 - Q_i)} {4(e^{\xi_i} + 1)^2 \xi_i^2} := 0 \iff \xi_i^2 = \pm Q_i \Rightarrow \xi_i := +\sqrt{Q_i}
$$
As an interpretation, $\xi_i^2$ is the second moment of the linear predictor $x_i^T \mathbf{b}$ under the current estimates for $q_l$.

With these updates derived, we can now implement the algorithm.


### A Note on the Intercept
In regular SuSiE, we can center our covariates and response to avoid fitting the intercept. Since our data is now 0/1, we can no longer center our response. Instead, we can fit an intercept in our IBFS.

I have not given much thought yet as to how an intercept should be included. I suspect we can simply add it to the lower bound and optimize it like we did for the $\xi_i$ (this approach should allow us to add any number of other predictors that we want to include but not penalize with a prior, e.g. adding control covariates such as gender, top 10 PCs, etc).

## Algebraic Comparison between VB and GLM Algorithms
In that follows, I will refer to the new VB approach as the **VB Algorithm**, and the original iterative GLM approach as the **GLM Algorithm**.

The form of the updates in both algorithms are very similar. This is not surprising, since in both cases the posterior distribution takes the same form: a multinoulli draw for the non-zero element of $b_l$, and then a normal draw for the corresponding effect size.

In the SER step of the GLM algorithm, we obtain MLE estimates for $\hat{b_j}$ and its SE $\sigma_{MLE_j}$. We then compute the posterior mean and variance of the normal distribution:
$$
\begin{aligned}
b|\gamma_j = 1 \sim \mathcal{N}(\mu_{1j}, \sigma_{1j}^2) \\
\mu_{1j} = \frac{\sigma_{1j}^2}{\sigma_{MLE_j}^2}\hat{b_j} \\
\sigma_{1j}^2 = \frac{1}{1/\sigma_0^2 + 1/\sigma_{MLE_j}^2}
\end{aligned}
$$

In the updates from the VB algorithm, we get
$$
\begin{aligned}
b|\gamma_j = 1 \sim \mathcal{N}(\mu_{1j}, \sigma_{1j}^2) \\
\mu_{1j} = \frac{\sum_{i=1}^n x_{ij}\Big(y_i - \frac{1}{2} - \frac{1}{\xi_i}(g(\xi_i) - \frac{1}{2})(x_i^T \sum_{k \ne l}\bar{b_k})\Big)}{1/\sigma_0^2 + \sum_{i=1}^n \frac{1}{\xi_i}(g(\xi_i) - \frac12)(x_{ij}^2)} \\
\sigma_{1j}^2 = \frac{1}{1/\sigma_0^2 + \sum_{i=1}^n \frac{1}{\xi_i}(g(\xi_i) - \frac12)(x_{ij}^2)} \\
\end{aligned}
$$

In order to connect these two update rules, recall what we actually do in the GLM case for updating $b_l$: 

1. We run a logistic regression of $Y \sim X_j$ keeping $b_{-l}$ fixed at its posterior mean, $\bar{b_{-l}}$. This gives us $\widehat{b_{lj, MLE}}$ and $\sigma_{lj, MLE}^2$;

2. We then find the posterior distribution using the asymptotic normality of the MLE.

Let's see what happens if we apply this same method, but instead of using the true logistic likelihood and find the MLE, instead use our variational lower-bound on the likelihood $h(b_l, \bar{b_{-l}}, \xi)$. Since we are essentially regression on just one $X_j$ in this step, $b_l$ will take the form $c \cdot e_j$ for basis vector $e_j$ and effect size $c$ which has a prior distribution of $\mathcal{N}(0, \sigma_0^2)$.

$$
\begin{aligned}
\log(h(ce_j, \bar{b_{-l}}, \xi)) = \sum_{i=1}^n \log(g(\xi_i)) + y_ix_i^T\bar{b_{-l}} + y_icx_{ij} - \frac{x_i^T\bar{b_{-l}} + cx_{ij} + \xi_i}{2} - \frac{1}{2\xi_i}((g(\xi_i) - \frac12)((x_i^T\bar{b_{-l}} + cx_{ij})^2 - \xi_i^2) \Rightarrow \\
\frac{\partial}{\partial c} \log(h(ce_j, \bar{b_{-l}}, \xi)) = \sum_{i=1}^n y_i x_{ij} - \frac{x_{ij}}{2} - \frac{1}{2\xi_i}(g(\xi_i) - \frac12)2x_{ij}(x_i^T\bar{b_{-l}} + cx_{ij}) := 0 \Rightarrow \\
\hat{c}_{MLE'} = \frac{\sum_{i=1}^n x_{ij}\Big[y_i - \frac12 - \frac{1}{\xi_i}(g(\xi_i) - \frac12)x_i^T\bar{b_{-l}}\Big]}{\sum_{i=1}^n x_{ij}^2 \frac{1}{\xi_i}(g(\xi_i) - \frac12)}
\end{aligned}
$$
Furthermore, if this were actually the likelihood, we could look at the second derivative to tell us about the variance:
$$
\frac{\partial^2}{\partial c^2} \log(h(ce_j, \bar{b_{-l}}, \xi)) = \sum_{i=1}^n-\frac{1}{\xi_i}(g(\xi_i) - \frac12)x_{ij}^2 \Rightarrow \sigma_{MLE'}^2 = \frac{1}{\sum_{i=1}^n\frac{1}{\xi_i}(g(\xi_i) - \frac12)x_{ij}^2}
$$

If we proceed in the same manner as in the GLM updates, we would get:
$$
\begin{aligned}
\mu_{1j} = \frac{\sigma_{1j}^2}{\sigma_{MLE'}^2}\hat{c}_{MLE'} = \frac{\sum_{i=1}^n\frac{1}{\xi_i}(g(\xi_i) - \frac12)x_{ij}^2}{1 / \sigma_0^2 + \sum_{i=1}^n\frac{1}{\xi_i}(g(\xi_i) - \frac12)x_{ij}^2} \cdot \frac{\sum_{i=1}^n x_{ij}\Big[y_i - \frac12 - \frac{1}{\xi_i}(g(\xi_i) - \frac12)x_i^T\bar{b_{-l}}\Big]}{\sum_{i=1}^n x_{ij}^2 \frac{1}{\xi_i}(g(\xi_i) - \frac12)} = \frac{\sum_{i=1}^n x_{ij}\Big(y_i - \frac{1}{2} - \frac{1}{\xi_i}(g(\xi_i) - \frac{1}{2})(x_i^T \sum_{k \ne l}\bar{b_k})\Big)}{1/\sigma_0^2 + \sum_{i=1}^n \frac{1}{\xi_i}(g(\xi_i) - \frac12)(x_{ij}^2)} \\
\sigma_{1j}^2 = \frac{1}{1/\sigma_0^2 + 1/\sigma_{MLE_j}^2} = \frac{1}{1 / \sigma_0^2 + \sum_{i=1}^n\frac{1}{\xi_i}(g(\xi_i) - \frac12)x_{ij}^2}
\end{aligned}
$$

Note that these are exactly the same updates we get from the VB method.


As for calculating the PIPs, in the GLM algorithm, we have
$$
\alpha_j \propto \pi_j \frac{\mathcal{N}(\hat{b_j}; 0, \sigma_0^2 + \sigma_{MLE_j}^2)}{\mathcal{N}(\hat{b_j}; 0, \sigma_{MLE_j}^2 )} = \pi_j \sqrt{\frac{\sigma_{MLE_j}^2}{\sigma_0^2 + \sigma_{MLE_j}^2}} \exp\Bigg\{\frac{\hat{b_j}^2}{2}\Big(\frac{1}{\sigma_{MLE_j}^2} - \frac{1}{\sigma_0^2 + \sigma_{MLE_j}^2}\Big)\Bigg\}
$$

In the VB algorithm, we have
$$
\alpha_j \propto \pi_j \sqrt{\sigma_{1j}^2} \exp\Bigg\{\frac{\mu_{1j}^2}{2\sigma_{1j}^2}\Bigg\}
$$

Now, if we start from the PIPs from the GLM version and plug in our results when applied to the VB algorithm:
$$
\begin{aligned}
\alpha_j \propto \pi_j \sqrt{\frac{\sigma_{MLE_j}^2}{\sigma_0^2 + \sigma_{MLE_j}^2}} \exp\Bigg\{\frac{\hat{c_j}^2}{2}\Big(\frac{1}{\sigma_{MLE_j}^2} - \frac{1}{\sigma_0^2 + \sigma_{MLE_j}^2}\Big)\Bigg\} = \pi_j \sqrt{\frac{1}{\sigma_0^2/\sigma_{MLE_j}^2 + 1}} \exp\Bigg\{\frac{\hat{c}_j^2}{2}\Big(\frac{\sigma_0^2}{(\sigma_{MLE_j}^2)(\sigma_0^2 + \sigma_{MLE_j}^2)}\Big)\Bigg\} = \\
[\hat{c}_j^2 = \mu_{1j}^2 \frac{\sigma_{MLE_j}^4}{\sigma_{1j}^4}] = \pi_j \sqrt{\frac{1}{\sigma_0^2(1/\sigma_{MLE_j}^2 + 1 / \sigma_0^2)}}\exp\Bigg\{\mu_{1j}^2 \frac{\sigma_{MLE_j}^4}{2\sigma_{1j}^4}\Big(\frac{\sigma_0^2}{(\sigma_{MLE_j}^2)(\sigma_0^2 + \sigma_{MLE_j}^2)}\Big)\Bigg\} \propto \\
[\sqrt{1 / \sigma_0^2} \; \text{constant}, \sigma_{1j}^2 = \frac{1}{1/\sigma_0^2 + 1/\sigma_{MLE_j}^2}]  \propto \pi_j \sqrt{\sigma_{1j}^2} \exp\Bigg\{\frac{\mu_{1j}^2}{2\sigma_{1j}^2} \cdot \Big(\frac{\sigma_0^2 \sigma_{MLE_j}^2}{\sigma_{1j}^2(\sigma_0^2 + \sigma_{MLE_j}^2)}\Big)\Bigg\} = \\
[\sigma_{1j}^2 = \frac{1}{1/\sigma_0^2 + 1/\sigma_{MLE_j}^2}] = \pi_j \sqrt{\sigma_{1j}^2} \exp\Bigg\{\frac{\mu_{1j}^2}{2\sigma_{1j}^2} \cdot \Big(\frac{\sigma_0^2 \sigma_{MLE_j}^2(1/\sigma_0^2 + 1/\sigma_{MLE_j}^2)}{\sigma_0^2 + \sigma_{MLE_j}^2}\Big)\Bigg\} = \\
\pi_j \sqrt{\sigma_{1j}^2} \exp\Bigg\{\frac{\mu_{1j}^2}{2\sigma_{1j}^2} \cdot \Big(\frac{\sigma_0^2 + \sigma_{MLE_j}^2}{\sigma_0^2 + \sigma_{MLE_j}^2}\Big)\Bigg\} = \pi_j \sqrt{\sigma_{1j}^2} \exp\Bigg\{\frac{\mu_{1j}^2}{2\sigma_{1j}^2}\Bigg\}
\end{aligned}
$$
This is the same form we get directly from the VB update.

This seems like magic at first! But when we think about it, the VB lower-bound we use makes the pseudo-likelihood $h$ a quadratic, so of course we should recover the original SuSiE rules (which we co-opted for use in the GLM algorithm).

So naturally, one might ask what the problem is with the GLM version. Since it uses the same general method and uses the true likelihood function, shouldn't it be better?

The answer lies in our fixing the other parameters to their posterior mean, $\bar{b}_{-l}$. In the update to $q_l$, we set
$$
q_l(c \cdot e_j) \propto \mathbb{E}_{-(q_l)}[\log(p(y|b_l = c \cdot e_j, b_{-l}) p(b_l = c \cdot e_j)) = \mathbb{E}_{-(q_l)}\Big[\log(\pi_j) + \log\Big(\mathcal{N}(c; 0, \sigma_0^2)\Big) + \sum_{i=1}^n y_ix_i^T(b_l + \sum_{k \ne l} b_k) - \log\Big(1 + \exp\{x_i^T(b_l + \sum_{k \ne l} b_k\}\Big)\Big]
$$
The issue with the GLM approach is that by finding the MLE for $b_l$ by treating $b_{-l}$ fixed at its posterior mean under $q_{-l}$ is that we are essentially saying that $\mathbb{E}_{-(q_l)}\Big[\log\Big(1 + \exp\{x_i^T(b_l + \sum_{k \ne l} b_k\}\Big)\Big] = \log\Big(1 + \exp\{x_i^T(b_l + \bar{b}_{-l}\}\Big)$. If it were the case that $\mathbb{E}_{-(q_l)}\Big[\log\Big(1 + \exp\{x_i^T(b_l + \sum_{k \ne l} b_k\}\Big)\Big] \le \log\Big(1 + \exp\{x_i^T(b_l + \bar{b}_{-l}\}\Big)$, then we would have no issue, since we would be maintaining the chain of "$\ge$" used in the derivation of VB (subtracting a smaller value vs. a larger value in the likelihood). However, $\log(1 + e^x)$ is a concave function, so by Jensen's inequality, $\mathbb{E}_{-(q_l)}\Big[\log\Big(1 + \exp\{x_i^T(b_l + \sum_{k \ne l} b_k\}\Big)\Big] \ge \log\Big(1 + \exp\{x_i^T(b_l + \bar{b}_{-l}\}\Big)$ (the opposite of what we want).

We would have approximate equality in the above case if $\log(1 + e^x)$ were roughly linear. The second derivative of $\log(1 + e^x)$ is $\frac{e^x}{(1 + e^x)^2}$. This second derivative has a maximum of $\frac14$ at $x = 0$, and looks like this:
```{r}
curve(exp(x) / ((1 + exp(x))^2), -5, 5, xlab = "x", ylab = "Second Derivative")
```
For larger values of $|x|$, the second derivative is quite small, and thus $\log(1+e^x)$ is roughly linear, so taking the expectation insude the function doesn't change things too much. And even at the maximum of 0, the second derivative is still on the smaller side. So one could argue that our implicit approximation in the GLM algorithm works well, even if the inequality is technically in the wrong direction.

The benefit of the VB approach is that we retain some information about the higher moments of $b_{-l}$ in the form of our $\xi_i$ variational parameters.

With this in mind, I suspect that if differences in the method can be found, it will be when the linear predictors $x_i^T\mathbf{b}$ are all close to 0.

# Implementation
The code below is a simplistic implementation of the above ideas.


```{r}
### NOTE: This is basic code, and I did not attempt to mirror the level of numerical sophistication in the susie functions
### If this idea is worth pursuing further, then this code can be improved

calc_Q = function(X, Sigma2, Mu, Alpha) {
  # X is data matrix
  # Sigma2[j, l] is the posterior variance for b_l when entry j is selected, p x L
  # Mu[j, l] is the posterior mean for b_l when entry j is selected, p x L
  # Alpha[j, l] is the posterior probability selecting entry j from b_l, p x L
  
  ASU2 = Alpha * (Sigma2 + Mu^2) # [j, l] = alpha[j, l] * (Sigma2[j, l] + Mu[j, l]^2)
  
  Q = rowSums(X^2 %*% ASU2) # start w/ sum_l E_(ql)[(x_i' b_l)^2]
  
  # now add 2 sum_l sum_{k>l} (x_i' b_l_post)(x_i' b_k_post)
  b_post_mat = Mu * Alpha # each column is posterior mean for b_l, p x L
  X_b_post = X %*% b_post_mat # [i, l] = x_i' b_l_post
  for (i in 1:nrow(X)) {
    xb_i = as.numeric(X_b_post[i, ])
    xb_outer = xb_i %o% xb_i # outer-product, [j, k] = (x_i'b_j_post)(x_i' b_k_post)
    diag(xb_outer) = 0 # remove diagonal, where j = k
    Q[i] = Q[i] + sum(xb_outer)
  }
  
  return(Q)
  
}

g = function(x) { # ilogit function
  exp(x) / (1 + exp(x))
}


update_xi = function(X, Sigma2, Mu, Alpha) {
  # X is data matrix
  # Sigma2[j, l] is the posterior variance for b_l when entry j is selected, p x L
  # Mu[j, l] is the posterior mean for b_l when entry j is selected, p x L
  # Alpha[j, l] is the posterior probability selecting entry j from b_l, p x L
  
  Q = calc_Q(X, Sigma2, Mu, Alpha)
  
  xi = sqrt(Q)
  
  return(xi)
  
}

update_b_l = function(X, Y, xi, prior_weights, V, Sigma2, Mu, Alpha, l) {
  # X is data matrix
  # Y is binary response
  # xi is lower-bound approximation parameters
  # prior_weights is prior probabilities for selecting j (p-vector)
  # V is prior variance
  # Sigma2[j, l] is the posterior variance for b_l when entry j is selected, p x L
  # Mu[j, l] is the posterior mean for b_l when entry j is selected, p x L
  # Alpha[j, l] is the posterior probability selecting entry j from b_l, p x L
  # l is index to update
  
  b_post_mat = Mu * Alpha # each column is posterior mean for b_l, p x L
  b_post_not_l = rowSums(as.matrix(b_post_mat[, -l], nrow = nrow(Mu))) # posterior, sum_(k != l) b_k_post
  g_xi = g(xi) # vector of g(xi_i), pre-compute once
  
  
  for (j in 1:ncol(X)) {
    common_denom = (1 / V) + sum(as.numeric(X[, j]^2) * (g_xi - .5) / xi) # appears many times, compute once
    
    # update Alpha[j, l]
    num = sum(X[, j] * (Y - .5 -  ((1 / xi) * (g_xi - .5) * (X %*% b_post_not_l)))) # numerator in exp
    denom = 2*common_denom # denominator in exp
    Alpha[j, l] = log(prior_weights[j]) + (num^2 / denom) + (1/2)*log(1 / common_denom) # on log-scale, for stability
    
    # update Mu[j, l]
    Mu[j, l] = num / common_denom
    
    # update Sigma[j, l]
    Sigma2[j, l] = 1 / common_denom
  }
  Alpha[, l] = exp(Alpha[, l] - max(Alpha[, l])) # remove max for stability, everything still proportional
  Alpha[, l] = Alpha[, l] / sum(Alpha[, l]) # normalize, sum to 1
  
  return(list(Sigma2 = Sigma2, Mu = Mu, Alpha = Alpha))
  
}


update_all = function(X, Y, xi, prior_weights, V, Sigma2, Mu, Alpha) {
  # X is data matrix
  # Y is binary response
  # xi is lower-bound approximation parameters
  # prior_weights is prior probabilities for selecting j (p-vector)
  # V is prior variance
  # Sigma2[j, l] is the posterior variance for b_l when entry j is selected, p x L
  # Mu[j, l] is the posterior mean for b_l when entry j is selected, p x L
  # Alpha[j, l] is the posterior probability selecting entry j from b_l, p x L
  
  # first, iterate over l = 1:L
  for (l in 1:ncol(Mu)) {
    res_l = update_b_l(X, Y, xi, prior_weights, V, Sigma2, Mu, Alpha, l)
    Sigma2 = res_l$Sigma2
    Mu = res_l$Mu
    Alpha = res_l$Alpha
  }
  
  # now, update xi
  xi = update_xi(X, Sigma2, Mu, Alpha)
  
  return(list(Sigma2 = Sigma2, Mu = Mu, Alpha = Alpha, xi = xi))
  
}

init_all = function(n, p, L) {
  xi = runif(n)
  
}


susie_logistic_VB = function(Y, X, L = 10, V = 1, prior_weights = NULL, tol = 1e-3, maxit = 1000) {
  p = ncol(X)
  n = nrow(X)
  
  if (is.null(prior_weights)) {
    prior_weights = rep(1 / p, p)
  }
  
  # place to store posterior info for each l = 1, ..., L
  # initialize: could think of something better
  Alpha = matrix(prior_weights, nrow = p, ncol = L)
  Mu = matrix(0, nrow = p, ncol = L)
  Sigma2 = matrix(V, nrow = p, ncol = L)
  xi = update_xi(X, Sigma2, Mu, Alpha)
  post_info = list(Sigma2 = Sigma2, Mu = Mu, Alpha = Alpha, xi = xi)
  
  beta_post_init = matrix(Inf, nrow = p, ncol = L) # initialize
  #beta_post_init2 = beta_post_init
  beta_post = post_info$Alpha * post_info$Mu
  
  
  iter = 0
  #while((norm(beta_post - beta_post_init, "1") > tol) & (norm(beta_post - beta_post_init2, "1") > tol)) { # repeat until posterior means
  while(norm(beta_post - beta_post_init, "1") > tol) { # repeat until posterior means
    #beta_post_init2 = beta_post_init # store from 2 iterations ago
    beta_post_init = beta_post
    
    post_info = update_all(X, Y, post_info$xi, prior_weights, V, post_info$Sigma2, post_info$Mu, post_info$Alpha)
    
    beta_post = post_info$Alpha * post_info$Mu
    
    iter = iter + 1
    if (iter > maxit) {
      stop("Maximum number of iterations reached")
    }
  }
  
  # change output format to match the GLM version's output
  res = list(post_alpha = post_info$Alpha, post_mu = post_info$Mu, post_sigma = post_info$Sigma, xi = post_info$xi)
  
  return(res)
  
}
```

I also include the code for the original iterative logistic SuSiE method below in order to compare the two methods.
**(NOTE: I will clean this up later by adding the code to the "Code" folder in the workflowR folder)**
```{r}
susie_logistic = function(Y, X, L = 10, V = 1, prior_weights = NULL, tol = 1e-3, maxit = 1000, intercept = TRUE) {
  
  p = ncol(X)
  n = nrow(X)
  
  # place to store posterior info for each l = 1, ..., L
  post_alpha = matrix(NA, nrow = p, ncol = L)
  post_mu = matrix(NA, nrow = p, ncol = L)
  post_sigma = matrix(NA, nrow = p, ncol = L)
  post_info = list(post_alpha = post_alpha, post_mu = post_mu, post_sigma = post_sigma)

  beta_post_init = matrix(Inf, nrow = p, ncol = L) # initialize
  beta_post_init2 = beta_post_init
  beta_post = matrix(0, nrow = p, ncol = L)
  
  fixed = rep(0, n) # fixed portion, estimated from l' != l other SER models
  
  iter = 0
  while((norm(beta_post - beta_post_init, "1") > tol) & (norm(beta_post - beta_post_init2, "1") > tol)) { # repeat until posterior means converge (ELBO not calculated here, so use this convergence criterion instead)
    beta_post_init2 = beta_post_init # store from 2 iterations ago
    beta_post_init = beta_post
      
    for (l in 1:L) {
      
      # below is old (inefficient) calculation of the fixed portion
      #fixed = rowSums(X %*% beta_post[, -l]) + int.coef # fixed portion from previous estimates (add intercept portion as well)
      fixed = fixed - (X %*% beta_post[, l]) # remove effect from previous iteration
      
      SER_logistric_l = single_effect_regression_logistic(Y, X, V, prior_weights, FALSE, fixed, intercept)
      # store
      post_info$post_alpha[, l] = SER_logistric_l$alpha
      post_info$post_mu[, l] = SER_logistric_l$mu
      post_info$post_sigma[, l] = SER_logistric_l$mu2 - SER_logistric_l$mu^2
      
      # update beta_post
      beta_post[, l] = SER_logistric_l$alpha * SER_logistric_l$mu
      
      fixed = fixed + (X %*% beta_post[, l]) # add back new fixed portion
      
    }
    
    iter = iter + 1
    if (iter > maxit) {
        stop("Maximum number of iterations reached")
    }
    
  }
  
  # now, get intercept w/ MLE, holding our final estimate of beta to be fixed
  beta = rowSums(post_info$post_alpha * post_info$post_mu)
  int = coef(glm(Y ~ 1 + offset(X %*% beta), family = "binomial"))
  post_info$intercept = int
  
  return(post_info)
  
}


# SER_logistic function
single_effect_regression_logistic = function(Y, X, V, prior_weights = NULL, optimize_V = FALSE, fixed = NULL, intercept = TRUE) {
  p = ncol(X)
  
  betahat = numeric(p)
  shat2 = numeric(p)
  
  if (is.null(fixed)) { # fixed is components from previous SER fits
    fixed = rep(0, length(Y))
  }
  
  # NOTE: could parallelize loop below if desired
  for (j in 1:p) { # logistic regression on each column of X separately
    if (intercept) {
      log.fit = glm(Y ~ X[, j] + 1 + offset(fixed), family = "binomial") # fit w/ intercept
    } else {
      log.fit = glm(Y ~ X[, j] - 1 + offset(fixed), family = "binomial") # fit w/out intercept
    }
    log.fit.coef = summary(log.fit)$coefficients
    # NOTE: coerces "intercept" to be 0 or 1 to grab relevant row of glm coefficient output
    betahat[j] = ifelse(is.na(log.fit.coef[1 + intercept, 1]), 0, log.fit.coef[1 + intercept, 1]) # beta-hat MLE (if na, just set to 0)
    shat2[j] = ifelse(is.na(log.fit.coef[1 + intercept, 2]), Inf, log.fit.coef[1 + intercept, 2]^2) # (std errof beta-hat MLE)^2 (if na, just set to Inf)
  }
  
  if (is.null(prior_weights)) {
    prior_weights = rep(1 / p, p)
  }
  
  if(optimize_V) {
    stop("Optimizing for prior variance not yet implemented for logistic case")
    #if(loglik.grad(0, betahat, shat2, prior_weights) < 0) {
    #  V = 0
    #} else {
    ##V.o = optim(par=log(V),fn=negloglik.logscale,gr = negloglik.grad.logscale,betahat=betahat,shat2=shat2,prior_weights=prior_weights,method="BFGS")
    ##if(V.o$convergence!=0){
    ##  warning("optimization over prior variance failed to converge")
    ##}
    #  V.u = uniroot(negloglik.grad.logscale, c(-10, 10), extendInt = "upX", betahat = betahat, shat2 = shat2, prior_weights = prior_weights)
    #  V = exp(V.u$root)
    #}
  }
  
  lbf = dnorm(betahat, 0, sqrt(V + shat2), log = TRUE) - dnorm(betahat, 0, sqrt(shat2), log = TRUE)
  #log(bf) on each SNP
  
  lbf[is.infinite(shat2)] = 0 # deal with special case of infinite shat2 (eg happens if X does not vary)
  
  maxlbf = max(lbf)
  w = exp(lbf - maxlbf) # w is proportional to BF, but subtract max for numerical stability
  # posterior prob on each SNP
  w_weighted = w * prior_weights
  weighted_sum_w = sum(w_weighted)
  alpha = w_weighted / weighted_sum_w
  post_var = 1 / ((1 / shat2) + (1 / V)) # posterior variance
  post_mean = (1 / shat2) * post_var * betahat # posterior mean
  post_mean2 = post_var + post_mean^2 # posterior second moment
  # BF for single effect model
  lbf_model = maxlbf + log(weighted_sum_w)
  # NOTE: Need to double check below
  loglik = lbf_model + log(1/2)*length(Y) # loglik of 0/1 response Y under p = .5
  return(list(alpha = alpha, mu = post_mean, mu2 = post_mean2, lbf = lbf, lbf_model = lbf_model, V = V, loglik = loglik))
}
```


# Demonstration

## Easy Demonstration
### L = 1
As a very easy demonstration, let $n = 100, p = 10, L = 1$, and let all columns of $X$ be distributed $\mathcal{N}(0, 1)$. WLOG, we let the first element of $\mathbf{b}$ be the only non-zero element. We set $\sigma_0^2 = 1$.

We then simulated $\mathbf{y}$ from the specified bernoulli model,
$$
y_i \stackrel{\perp}{\sim} \text{Bern}\Bigg(\frac{e^{(\mathbf{Xb})_i}}{1 + e^{(\mathbf{Xb})_i}}\Bigg)
$$
(We do not add an intercept term here, since intercepts are not currently implemented in the VB method).

We repeat this procedure 100 times.

```{r}
set.seed(1138)

n = 100
p = 10
L = 1
V = 1
beta_true = rep(0, p)

B = 100
b_1s = numeric(B)
susie.fits.easy = list() # logistic SuSiE fits
logistic.fits.easy = list() # logistic regression fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  # make response
  Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
  susie.fits.easy[[i]] = susie_logistic_VB(Y, X, L, V)
  logistic.fits.easy[[i]] = glm(Y ~ X, family = "binomial")
}
```

Figure 1 below plots the estimated value for $\hat{b}_1 := \mu_1 \cdot \alpha_1$ against the true value, where $\mu_1$ is the estimated posterior mean, and $\alpha_1$ is the estimated posterior inclusion probability. Figure 2 below plots the PIP, $\alpha_i$, against the true value of $b_1$.
```{r}
plot(sapply(susie.fits.easy, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Estimate", main = "Figure 1", pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy, function(x) x$post_alpha[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Inclusion Probability", main = "Figure 2", pch = 19)
```
Figure 1 shows that, in this simulation, we observe a shrinkage behavior similar to hard-thresholding (with a small thresholding value). We also see that our estimated lie pretty close to the truth. Figure 2 shows a sharp transition between a very low PIP and a PIP near 1. This sharp transition is what causes the hard-thresholding-like shrinkage we obsersve in figure 1: the posterior mean values $\mu_1$ closely match the true values for $b_1$, but are effectively shrunk to 0 in this region by the PIP $\alpha_i$, and are effectively untouched outside this region.

We can compare figure 2 with the p-values obtained from the normal logistic regressions on the same simulated data, shown in figure 3 below.
```{r}
plot(sapply(logistic.fits.easy, function(x) summary(x)$coefficients[2, 4]) ~ b_1s, xlab = "True Effect Value", ylab = "Logistic p-value", main = "Figure 3", pch = 19)
```
We see a similar phase transition around the same values of the true effect size, $|b_1| \le 0.5$.


### L = 2
We now repeat the same as above, except with $L = 2, \sigma_0^2 = 1$, and the second element $b_2$ also a non-zero.
```{r}
set.seed(1138)

n = 100
p = 10
L = 2
V = 1
beta_true = rep(0, p)

B = 100
b_1s = numeric(B)
b_2s = numeric(B)
susie.fits.easy2 = list() # logistic SuSiE fits
logistic.fits.easy2 = list() # logistic regression fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  beta_true[2] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  b_2s[i] = beta_true[2]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  # make response
  Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
  susie.fits.easy2[[i]] = susie_logistic_VB(Y, X, L, V, maxit = 1000)
  logistic.fits.easy2[[i]] = glm(Y ~ X, family = "binomial")
}
```


Figures 4(a-b) below plot the estimated value for $\hat{b}_1 := \sum_{l = 1}^2 \mu_{l,1} \cdot \alpha_{l,1}$ and $\hat{b}_2 := \sum_{l = 1}^2 \mu_{l,2} \cdot \alpha_{l,2}$ against the true value, where $\mu_{l,i}$ is the estimated posterior mean, and $\alpha_{l,i}$ is the estimated posterior inclusion probability, for entry $i$ in estimated vector $l$. Figures 5(a-b) below plot the maximum PIP, $\max_{l \in \{1, 2\}}\alpha_{l,i}$, against the true value of $b_i$.

```{r}
#est_b1_max = sapply(susie.fits.easy2, function(x) x$post_alpha[1, which.max(x$post_alpha[1, ]) ]* x$post_mu[1, which.max(x$post_alpha[1, ])])

#est_b2_max = sapply(susie.fits.easy2, function(x) x$post_alpha[2, which.max(x$post_alpha[2, ]) ]* x$post_mu[2, which.max(x$post_alpha[2, ])])

est_alpha1_max = sapply(susie.fits.easy2, function(x) max(x$post_alpha[1, ]))

est_alpha2_max = sapply(susie.fits.easy2, function(x) max(x$post_alpha[2, ]))

est_b = sapply(susie.fits.easy2, function(x) rowSums(x$post_alpha * x$post_mu))

plot(est_b[1, ] ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Estimate", main = "Figure 4(a) - Entry 1", pch = 19)
abline(0, 1)

plot(est_b[2, ] ~ b_2s, xlab = "True Effect Value", ylab = "Posterior Estimate", main = "Figure 4(b) - Entry 2", pch = 19)
abline(0, 1)

plot(est_alpha1_max ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Inclusion Probability", main = "Figure 5(a) - Entry 1", pch = 19)

plot(est_alpha2_max ~ b_2s, xlab = "True Effect Value", ylab = "Posterior Inclusion Probability", main = "Figure 5(b) - Entry 2", pch = 19)
```
We see a similar pattern as above, in the case $L = 1$. The method is also able to detect both effects (when strong enough).

We can compare figures 5(a-b) with the p-values obtained from the normal logistic regressions on the same simulated data, shown in figures 6(a-b) below.
```{r}
plot(sapply(logistic.fits.easy2, function(x) summary(x)$coefficients[2, 4]) ~ b_1s, xlab = "True Effect Value", ylab = "Logistic p-value", main = "Figure 6(a) - Entry 1", pch = 19)

plot(sapply(logistic.fits.easy2, function(x) summary(x)$coefficients[3, 4]) ~ b_2s, xlab = "True Effect Value", ylab = "Logistic p-value", main = "Figure 6(b) - Entry 2", pch = 19)
```
We see a similar phase transition around the same values of the true effect size, $|b_i| \le 0.5$.


## Medium Demonstration
As a medium difficulty demonstration, let $n = 100, p = 10, L = 3, \sigma_0^2 = 5$.

We create the data $\mathbf{X}$ where all entries are iid standard normal. I then set the 2nd column to be identical to the 3rd, and the 7th column identical to the 6th (when running this simulation, $b_3$ and $b_6$ were generated to be non-zero, and $b_2$ and $b_7$ were generated to be 0, so this was done in the spirit of the toy example from the paper).

We then simulated $\mathbf{y}$ from the specified bernoulli model,
$$
y_i \stackrel{\perp}{\sim} \text{Bern}\Bigg(\frac{e^{(\mathbf{Xb})_i}}{1 + e^{(\mathbf{Xb})_i}}\Bigg)
$$

```{r}
### TEST susie_logistic_VB
set.seed(1138)

n = 100
p = 10
L = 3
pi = rep(1 / p, p) # prior weights
V = 5 # prior variance

beta_true = rep(0, p)
for (l in 1:L) {
  b_l = rnorm(1, 0, sqrt(V))
  gamma_l = rmultinom(1, 1, pi)
  beta_l = b_l * gamma_l
  beta_true = beta_true + beta_l
}

# simulate data, induce correlations
X = matrix(rnorm(n*p, 0, 1), nrow = n, ncol = p)
X[, 2] = X[, 3]
X[, 7] = X[, 6]

# make response
Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))

susie.logistic.fit = susie_logistic_VB(Y, X, L, V)
```

Figure 11 below plots the true values for $\mathbf{b}$.
```{r}
plot(beta_true, ylab = "True Value", main = "Figure 11", pch = 19)
```
The 3rd and 6th are around -1.3, and the 5th is around 0.75.

Figures 12(a-c) below plots the posterior means for the 3 vectors estimated from this procedure (calculated as $\mathbf{\bar{b}}_l := \alpha_l \circ \mu_{1l}$ using $\alpha_l, \mu_{1l}$ returned from the logistic-version of the IBFS algorithm):
```{r}
b_1_post = susie.logistic.fit$post_mu[, 1] * susie.logistic.fit$post_alpha[, 1]
b_2_post = susie.logistic.fit$post_mu[, 2] * susie.logistic.fit$post_alpha[, 2]
b_3_post = susie.logistic.fit$post_mu[, 3] * susie.logistic.fit$post_alpha[, 3]

plot(b_1_post, ylab = "Posterior Mean Value", main = "Figure 12(a)", pch = 19)
plot(b_2_post, ylab = "Posterior Mean Value", main = "Figure 12(b)", pch = 19)
plot(b_3_post, ylab = "Posterior Mean Value", main = "Figure 12(c)", pch = 19)
```
We can see that the first estimated vector captures our constructed correlation between the 2nd and 3rd columns, the second estimated vector captures our constructed correlation between the 6th and 7th columns, and the third estimated vector captures the individual effect of the 5th column.

Figures 13(a-c) below plots our estimated PIPs, $\alpha_1, \alpha_2, \alpha_3$:
```{r}
plot(susie.logistic.fit$post_alpha[, 1], ylab = "Posterior Inclusion Probability", main = "Figure 13(a)", pch = 19)
plot(susie.logistic.fit$post_alpha[, 2], ylab = "Posterior Inclusion Probability", main = "Figure 13(b)", pch = 19)
plot(susie.logistic.fit$post_alpha[, 3], ylab = "Posterior Inclusion Probability", main = "Figure 13(c)", pch = 19)
```
These groups of correlated predictors are shows from the PIPs.


## Hard Demonstration
As a hard demonstration, let $n = 1,000, p = 100, L = 10, \sigma_0^2 = 5$.

We create the data $\mathbf{X}$ where all entries are iid standard normal. I then set the 2nd column to be identical to the 1st, the 88th column identical to the 87th, and the 89th column highly negatively correlated with the 87th (when running this simulation, $b_1$ and $b_87$ were generated to be non-zero, and $b_2$, $b_{88}$ and $b_{89}$ were generated to be 0, so this was done in the spirit of the toy example from the paper).

We then simulated $\mathbf{y}$ from the specified bernoulli model,
$$
y_i \stackrel{\perp}{\sim} \text{Bern}\Bigg(\frac{e^{(\mathbf{Xb})_i}}{1 + e^{(\mathbf{Xb})_i}}\Bigg)
$$

```{r, cache = T}
set.seed(1138)

n = 1000
p = 100
L = 10
pi = rep(1 / p, p) # prior weights
V = 5 # prior variance

beta_true = rep(0, p)
for (l in 1:L) {
  b_l = rnorm(1, 0, sqrt(V))
  gamma_l = rmultinom(1, 1, pi)
  beta_l = b_l * gamma_l
  beta_true = beta_true + beta_l
}

# simulate data, induce correlations
X = matrix(rnorm(n*p, 0, 1), nrow = n, ncol = p)
X[, 2] = X[, 1]
X[, 88] = X[, 87]
X[, 89] = runif(n, -1, -.7) * X[, 87]

# make response
Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))

susie.logistic.fit = susie_logistic_VB(Y, X, L, V)
```

Figure 14 below plots the true values for $\mathbf{b}$ (the numbers correspond to the points and indices of non-zero true effects).
```{r}
plot(beta_true[beta_true == 0] ~ which(beta_true == 0), xlim = c(0, 101), ylim = range(beta_true) + c(-.1, .1), xlab = "Index", ylab = "True Value", main = "Figure 14", pch = 19)
text(which(beta_true != 0), beta_true[beta_true != 0], labels = which(beta_true != 0))
```
We can see that true effects 41 and 56 are very small, and 27, 60, and 87 are also small-ish.

Figures 15(a-j) below plots the posterior means for the 10 vectors estimated from this procedure (calculated as $\mathbf{\bar{b}}_l := \alpha_l \circ \mu_{1l}$ using $\alpha_l, \mu_{1l}$ returned from the logistic-version of the IBFS algorithm):
```{r, fig.height = 12}
par(mfrow = c(5, 2))
for (l in 1:L) {
  b_l_post = susie.logistic.fit$post_mu[, l] * susie.logistic.fit$post_alpha[, l]
  plot(b_l_post[abs(b_l_post) < .01] ~ which(abs(b_l_post) < .01), xlim = c(0, 101), ylim = range(b_l_post) + c(-.1, .1), xlab = "Index", ylab = "Posterior Mean Value", main = paste("Figure 15(", letters[l], ")", sep = ""), pch = 19)
  text(which(abs(b_l_post) >= .01), b_l_post[abs(b_l_post) >= .01], labels = which(abs(b_l_post) >= .01))
}
par(mfrow = c(1, 1))
```
(NOTE: In figures 15(a) and 15(h), 1 and 2 are close together, so it looks like the number 12. But it is really just 1 and 2).

We can see that the first estimated vector (15a) (and 8th, 15(h)) captures our constructed correlation between the 1st and 2nd columns, and 8th vector (15h) captures our constructed correlation between the 87th, 88th, and 89th columns (note the signs: 89 was constructed to be negatively correlated with the true effect column 87).


Figures 16(a-j) below plots our estimated PIPs, $\alpha_1, \alpha_2, \alpha_3$:
```{r, fig.height = 12}
par(mfrow = c(5, 2))
for (l in 1:L) {
  alpha_l = susie.logistic.fit$post_alpha[, l]
  plot(alpha_l[alpha_l < .01] ~ which(alpha_l < .01), xlim = c(0, 101), ylim = c(0, 1), xlab = "Index", ylab = "Posterior Inclusion Probability", main = paste("Figure 16(", letters[l], ")", sep = ""), pch = 19)
  text(which(alpha_l >= .01), alpha_l[alpha_l >= .01], labels = which(alpha_l >= .01))
}
par(mfrow = c(1, 1))
```
These groups of correlated predictors are shows from the PIPs.


## Misspecified $L$
In regular SuSiE, the algorithm is robust to misspecifying the value for $L$. For instance, say there are 3 true groups of correlated predictors. Then even we specify, say, $L = 5$, the algorithm will give us 3 meaningful credible sets and 7 impure ones (see sections 3.4.2 and 3.5 of the SuSiE paper). In this section, I show via example that this appears to hold true in the logistic version as well.

For example, consider the easy case where $n = 1000$, $p = 10$, each entry $X_{ij} \stackrel{iid}{\sim} \mathcal{N}(0, 1)$, and the first three effects are $b_1 = 1, b_2 = -0.75, b_3 = 0.5$. I test when I set $L = 3$ (the optimal value), $L = 5$, and $L = 10$.

```{r, fig.height = 12}
set.seed(1138)

n = 1000
p = 10
L = 1
beta_true = rep(0, p)
beta_true[1:3] = c(1, -.75, .5)
V = 1

X = matrix(rnorm(n * p), nrow = n)
# make response
Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))

fit.susie.log3 = susie_logistic_VB(Y, X, L = 3, V, maxit = 1000)
fit.susie.log5 = susie_logistic_VB(Y, X, L = 5, V, maxit = 1000)
fit.susie.log10 = susie_logistic_VB(Y, X, L = 10, V, maxit = 1000)

par(mfcol = c(3, 2))
matplot(fit.susie.log3$post_alpha, xlab = "Index", ylab = "Posterior Inclusion Probability", main = "L = 3")
matplot(fit.susie.log5$post_alpha, xlab = "Index", ylab = "Posterior Inclusion Probability", main = "L = 5")
matplot(fit.susie.log10$post_alpha, xlab = "Index", ylab = "Posterior Inclusion Probability", main = "L = 10")

plot(rowSums(fit.susie.log3$post_alpha * fit.susie.log3$post_mu), xlab = "Index", ylab = "Posterior Mean", main = "L = 3", pch = 19)
plot(rowSums(fit.susie.log5$post_alpha * fit.susie.log5$post_mu), xlab = "Index", ylab = "Posterior Mean", main = "L = 5", pch = 19)
plot(rowSums(fit.susie.log10$post_alpha * fit.susie.log10$post_mu), xlab = "Index", ylab = "Posterior Mean", main = "L = 10", pch = 19)
par(mfcol = c(1, 1))
```

We can see that having a value of $L$ too large adds groups with a lot of impurity in the PIPs (these groups being roughly identical, which is peculiar to me). Unsurprisingly, since our PIPs are forced to sum to 1, adding more groups will always add to the posterior means of each coefficient (assuming effects from different groups don't cancel out due to having opposite signs). We can see this on the right side of the plots above (for $L = 3$, coefficients 4-10 are close to 0, but for $L = 5$ and $L = 10$, these effects start to deviate from 0).

## Comparison Between the VB Algorithm and the Original Iterative GLM Modification of Regular SuSiE
In that follows, I will refer to the new VB approach as the **VB Algorithm**, and the original iterative GLM approach as the **GLM Algorithm**.

**NOTE: I have so far been unable to find many meaningful differences between the two algorithms**

### Convergence
First, before diving into any simulation examples, recall that for the GLM algorithm, I encountered situations where the algorithm didn't converge (e.g. cycling behavior). To remedy this, I modified the convergence criteria to also take into account the estimated means from 2 iterations prior (not just the previous iteration). In the VB algorithm, since it is a hill climbing algorithm, we should not encounter scenarios where the algorithm fails to converge.

### Easy Comparison
Here, I use the same simulations as from the "Easy Demonstration" section above. We have $n = 100$, $p = 10$, $L = 1$, $\sigma_0 = 1$, and set the first coefficient $b_1$ to be non-zero.

```{r, fig.height = 12}
set.seed(1138)

n = 100
p = 10
L = 1
V = 1
beta_true = rep(0, p)

B = 100
b_1s = numeric(B)
susie.fits.easy = list() # GLM logistic SuSiE fits
susie.fits.easy.VB = list() # VB logistic SuSiE fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  # make response
  Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
  susie.fits.easy[[i]] = susie_logistic(Y, X, L, V, intercept = F) # fit without intercept to compare directly w/ VB method
  susie.fits.easy.VB[[i]] = susie_logistic_VB(Y, X, L, V)
}

par(mfcol = c(3, 2))
plot(sapply(susie.fits.easy, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Estimate", main = "GLM SuSiE Coefficient Estimates: n = 100", pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy, function(x) x$post_alpha[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Inclusion Probability", main = "GLM SuSiE PIP Estimates: n = 100", pch = 19, ylim = c(0, 1))

plot(sapply(susie.fits.easy, function(x) x$post_mu[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Mu Estimate", main = "GLM SuSiE Coefficient Estimates: n = 100", pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy.VB, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Estimate", main = "VB SuSiE Coefficient Estimates: n = 100", pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy.VB, function(x) x$post_alpha[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Inclusion Probability", main = "VB SuSiE PIP Estimates: n = 100", pch = 19, ylim = c(0, 1))

plot(sapply(susie.fits.easy.VB, function(x) x$post_mu[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Mu Estimate", main = "VB SuSiE Coefficient Estimates: n = 100", pch = 19)
abline(0, 1)
par(mfcol = c(1, 1))
```

Qualitatively, these two methods appear nearly identical in this example. The only difference I see is in the PIPs for true effects around -1 to -1.25. The GLM method has two PIPs around 0.9, whereas almost all PIPs for the VB method are around 1.

To make things slightly harder, I perform the experiment again, except I not set $n = 50$ (so the effect should be harder to identify).

```{r, fig.height = 12}
set.seed(1138)

n = 50
p = 10
L = 1
V = 1
beta_true = rep(0, p)

B = 100
b_1s = numeric(B)
susie.fits.easy = list() # GLM logistic SuSiE fits
susie.fits.easy.VB = list() # VB logistic SuSiE fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  # make response
  Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
  susie.fits.easy[[i]] = susie_logistic(Y, X, L, V, intercept = F) # fit without intercept to compare directly w/ VB method
  susie.fits.easy.VB[[i]] = susie_logistic_VB(Y, X, L, V)
}

par(mfcol = c(3, 2))
plot(sapply(susie.fits.easy, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Estimate", main = "GLM SuSiE Coefficient Estimates: n = 50", pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy, function(x) x$post_alpha[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Inclusion Probability", main = "GLM SuSiE PIP Estimates: n = 50", pch = 19, ylim = c(0, 1))

plot(sapply(susie.fits.easy, function(x) x$post_mu[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Mu Estimate", main = "GLM SuSiE Coefficient Estimates: n = 50", pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy.VB, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Estimate", main = "VB SuSiE Coefficient Estimates: n = 50", pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy.VB, function(x) x$post_alpha[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Inclusion Probability", main = "VB SuSiE PIP Estimates: n = 50", pch = 19, ylim = c(0, 1))

plot(sapply(susie.fits.easy.VB, function(x) x$post_mu[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Mu Estimate", main = "VB SuSiE Coefficient Estimates: n = 50", pch = 19)
abline(0, 1)
par(mfcol = c(1, 1))
```

The differences here are more apparent. First, the GLM coefficient estimates for large true effects are biased towards 0 far more than the VB estimates. The driving factor is likely the PIPs, where the GLM version has a hard time; even for large effects, the PIPs are still only at aroung 0.9 at best. In contrast, the VB method returns PIPs of 1 for large enough effects.



### Hard Comparison
In this setting, I set $n = 100$, $p = 10$, and $\sigma_0 = 1$. I generate each observation of $X$ as coming from a multivariate normal distribution with mean $0$ and covariance structured such that all diagonal entries are 1, and for groups of correlated entries we get 0.95. The correlated groups are (1, 2, 3), (4, 5), and (9, 10). I then set the following entries in $\mathbf{b}$ to be non-zero: 1, 4, 6, 7 (two are in correlated groups, two are not, and there is a correlated group on zero effect variables).

```{r}
set.seed(1138)
n = 100
p = 10
V = 1
L = 4

Sigma = diag(.05, nrow = 10)
Sigma[1:3, 1:3] = Sigma[1:3, 1:3] + matrix(.95, nrow = 3, ncol = 3) # 1, 2, 3 correlated 
Sigma[4:5, 4:5] = Sigma[4:5, 4:5] + matrix(.95, nrow = 2, ncol = 2) # 4, 5 correlated
Sigma[6:8, 6:8] = Sigma[6:8, 6:8] + diag(.95, nrow = 3)
Sigma[9:10, 9:10] = Sigma[9:10, 9:10] + matrix(.95, nrow = 2, ncol = 2) # 9, 10 correlated

X = MASS::mvrnorm(n, rep(0, p), Sigma)
beta_true = rep(0, p)
beta_true[c(1, 4, 6, 7)] = rnorm(4, 0, sqrt(V))
Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))

ptm = proc.time()
susie.fit.glm = susie_logistic(Y, X, L, V, intercept = F)
ptm.glm = proc.time() - ptm
ptm = proc.time()
susie.fit.vb = susie_logistic_VB(Y, X, L, V)
ptm.vb =proc.time() - ptm

plot(beta_true[beta_true == 0] ~ which(beta_true == 0), xlim = c(0, 11), ylim = range(beta_true) + c(-.1, .1), xlab = "Index", ylab = "True Value", main = "True Effects", pch = 19)
text(which(beta_true != 0), beta_true[beta_true != 0], labels = which(beta_true != 0))


par(mfrow = c(2, 2))
for (l in 1:L) {
  b_l_post = susie.fit.glm$post_mu[, l] * susie.fit.glm$post_alpha[, l]
  plot(b_l_post[abs(b_l_post) < .01] ~ which(abs(b_l_post) < .01), xlim = c(0, 11), ylim = range(b_l_post) + c(-.1, .1), xlab = "Index", ylab = "Posterior Mean Value", main = paste("GLM SuSiE Posterior Mean No. ", l, sep = ""), pch = 19)
  text(which(abs(b_l_post) >= .01), b_l_post[abs(b_l_post) >= .01], labels = which(abs(b_l_post) >= .01))
}
par(mfrow = c(1, 1))

par(mfrow = c(2, 2))
for (l in 1:L) {
  b_l_post = susie.fit.vb$post_mu[, l] * susie.fit.vb$post_alpha[, l]
  plot(b_l_post[abs(b_l_post) < .01] ~ which(abs(b_l_post) < .01), xlim = c(0, 11), ylim = range(b_l_post) + c(-.1, .1), xlab = "Index", ylab = "Posterior Mean Value", main = paste("VB SuSiE Posterior Mean No. ", l, sep = ""), pch = 19)
  text(which(abs(b_l_post) >= .01), b_l_post[abs(b_l_post) >= .01], labels = which(abs(b_l_post) >= .01))
}
par(mfrow = c(1, 1))
```
From these plots, we see that the posterior mean estimates in both algorithms are sensible and nearly indistinguishable.

### Computation Time Comparison
In the previous example, the GLM algorithm took `r ptm.glm[3]` seconds to converge and the VB algorithm took `r ptm.vb[3]` seconds to converge (both with the same convergence tolerance). The VB method appears to be much faster. This is not surprising, since the GLM algorithm makes many calls the R's built-in `glm` function, which requires us to run an optimization algorithm. For the VB method, since we are essentially finding an analytical solution of an approximation to the same problem, the algorithm runs much faster.

**HOWEVER, I have found that in most cases, their runtimes are virtually the same**

