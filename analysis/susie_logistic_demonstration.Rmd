---
title: "susieR Logistic Regression GLM"
author: "Andrew Goldstein"
date: "December 3, 2018"
output:
  workflowr::wflow_html:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, error = T, warning = T, message = F, fig.align = 'center')
```

# Introduction
This page aims to explore an analog to SuSiE applied to 0/1 data using logistic regression. The basic idea is to replace the Single Effect Regression (SER) step in the Iterative Bayesian Forward Selection (IBFS) algorithm with an analogous logistic regression step.

Our response is $\mathbf{y} \in \mathbb{R}^n$, and our covariates are $\mathbf{X} \in \mathbb{R}^{n \times p}$.

## SER Model
In the standard SuSiE model, the following SER regression model is used:
$$
\begin{aligned}
\mathbf{y} = \mathbf{Xb} + \mathbf{e} \\
\mathbf{e} \sim \mathcal{N}(0, \sigma^2 I_n) \\
\mathbf{b} = \mathbf{\gamma}b \\
\mathbf{\gamma} \sim \text{Mult}(1, \mathbf{\pi}) \\
b \sim \mathcal{N}(0, \sigma_0^2)
\end{aligned}
$$

From this model, the standard OLS estimates (also the MLE) for $b_j$ is found using $p$ independent simple linear regressions (fit without an intercept), $\hat{b_j}(\mathbf{y}, \mathbf{x_j}) = (\mathbf{x_j^Tx_j})^{-1}\mathbf{x_j^Ty}$, with variance $\sigma_{MLE_j}^2 = \sigma^2 (\mathbf{x_j^Tx_j})^{-1}$. The posterior distribution for $b_j$ given $\gamma_j = 1$ is then
$$
\begin{aligned}
b|\gamma_j = 1, \mathbf{y}, \sigma^2, \sigma_0^2 \sim \mathcal{N}(\mu_{1j}, \sigma_{1j}^2) \\
\sigma_{1j}^2 = \frac{1}{1 / \sigma_{MLE_j}^2 + 1 / \sigma_0^2} \\
\mu_{1j} = \frac{1 / \sigma_{MLE_j}^2}{1 / \sigma_{MLE_j}^2 + 1 / \sigma_0^2} \hat{b_j}(\mathbf{y}, \mathbf{x_j}) + \frac{1 / \sigma_0^2}{1 / \sigma_{MLE_j}^2 + 1 / \sigma_0^2} 0 = \frac{\sigma_{1j}^2}{\sigma_{MLE_j}^2} \hat{b_j}(\mathbf{y}, \mathbf{x_j})
\end{aligned}
$$

The Bayes Factor is then calculated as
$$
\frac{p(\hat{b_j}(\mathbf{y}, \mathbf{x_j}) | b_j \sim \mathcal{N}(0, \sigma_0^2))}{p(\hat{b_j}(\mathbf{y}, \mathbf{x_j}) | b_j = 0)}
$$

Since $\hat{b_j}(\mathbf{y}, \mathbf{x_j}) \sim \mathcal{N}(b_j, \sigma_{MLE_j}^2) = b_j + \mathcal{N}(0, \sigma_{MLE_j}^2)$, we have that
$$
\begin{aligned}
\hat{b_j}(\mathbf{y}, \mathbf{x_j}) \sim_{M_{1j}} \mathcal{N}(0, \sigma_0^2 + \sigma_{MLE_j}^2) \\
\hat{b_j}(\mathbf{y}, \mathbf{x_j}) \sim_{M_{0_j}} \mathcal{N}(0, \sigma_{MLE_j}^2) \\
\end{aligned}
$$
where $M_{1j}$ is the specified model where $b_j$ is the non-zero element, and $M_{0j}$ is the null model (i.e. the model in which $b_j = 0$). From this, we see that the Bayes Factor is simply a ratio of normal densities.

Then, we calculate the vector of PIPs $\alpha$ as:
$$
\alpha_j = \frac{\text{BF}(\mathbf{y}, \mathbf{x_j}; \sigma^2, \sigma_0^2) \pi_j}{\sum_{k = 1}^p \text{BF}(\mathbf{y}, \mathbf{x_k}; \sigma^2, \sigma_0^2) \pi_k}
$$

Here, the following adjusted model is considered:
$$
\begin{aligned}
\mathbf{y} \sim \text{Bern}\Bigg(\frac{e^\mathbf{Xb}}{1 + e^{\mathbf{Xb}}}\Bigg) \quad \text{(element-wise)} \\
\mathbf{b} = \gamma b \\
\mathbf{\gamma} \sim \text{Mult}(1, \mathbf{\pi}) \\
b \sim \mathcal{N}(0, \sigma_0^2)
\end{aligned}
$$

From here, instead of computing the closed-form solution for $\hat{b_j}(\mathbf{y}, \mathbf{x_j})$, we perform a logistic regression with R's `glm` function in order to calculate $\hat{b_j}(\mathbf{y}, \mathbf{x_j})$ and $\sigma_{MLE_j}^2$ for each of the $p$ single-variable logistic regressions (fit without an intercept).

The rest follows as outlined above for the regular SER model.


## Full Model
The full SuSiE model is as follows:
$$
\begin{aligned}
\mathbf{y} = \mathbf{Xb} + \mathbf{e} \\
\mathbf{e} \sim \mathcal{N}(0, \sigma^2 I_n) \\
\mathbf{b} = \sum_{l = 1}^L \mathbf{b}_l \\
\mathbf{b}_l = \gamma_l b_l \quad (\text{independently for } l = 1, \dots, L) \\
\gamma_l \sim \text{Mult}(1, \mathbf{\pi}) \\
b_l \sim \mathcal{N}(0, \sigma_{0l}^2)
\end{aligned}
$$

The IBFS algorithm is then applied. In this algorithm, we iterate over $l \in \{1, \dots, L\}$ and perform our SER step on the residuals, $\mathbf{r_l} \equiv \mathbf{y} - \sum_{l' \ne l} \mathbf{X\bar{b}}_{l'}$, to get the vectors $\alpha_l, \mu_{1l}, \sigma_{1l}$. We then set $\mathbf{\bar{b}}_l := \alpha_l \circ \mu_{1l}$ (Schur product).


Here, the following adjusted model is considered:
$$
\begin{aligned}
\mathbf{y} \sim \text{Bern}\Bigg(\frac{e^\mathbf{Xb}}{1 + e^{\mathbf{Xb}}}\Bigg) \quad \text{(element-wise)} \\
\mathbf{b} = \sum_{l = 1}^L \mathbf{b}_l \\
\mathbf{b}_l = \gamma_l b_l \quad (\text{independently for } l = 1, \dots, L) \\
\gamma_l \sim \text{Mult}(1, \mathbf{\pi}) \\
b_l \sim \mathcal{N}(0, \sigma_{0l}^2)
\end{aligned}
$$

We modify the IBFS algorithm to use the analogs from this model. The main difference is that we can no longer perform SER on the residuals, since the residuals don't really make sense in the logistic setting.

However, noting that the following regressions are equivalent (where $\sim$ is interpreted as the regression formula, as in `lm` or `glm`), we have motivation for altering the IBFS algorithm:
$$ 
\mathbf{r}_l \sim \mathbf{Xb}_l  \iff \mathbf{y} - \sum_{l' \ne l} \mathbf{X\bar{b}}_{l'} \sim \mathbf{Xb}_l  \iff \mathbf{y} \sim \mathbf{Xb}_l + \text{offset}(\sum_{l' \ne l} \mathbf{X\bar{b}}_{l'})
$$
Here, `offset` is the function in R that forces the coefficient for the predictor to be 1.

Thus, in our modified logistic version, when fitting the SER each iteration, we can simply fit:
```
glm(Y ~ X[, j] + offset(fixed), family = "binomial")
```
separately for each $j \in \{1, \dots, p\}$, where `fixed` is $\sum_{l' \ne l} \mathbf{X\bar{b}}_{l'}$.


### A Note on the Intercept
In regular SuSiE, we can center our covariates and response to avoid fitting the intercept. Since our data is now 0/1, we can no longer center our response. Instead, we can fit an intercept in our IBFS.

However, we note that centering X and Y at the start of the procedure and not fitting an intercept in each SER is equivalent to not centering X and Y and fitting an intercept in each SER step. As a result, in the logistic case, we do fit an intercept in each logistic SER.

# Adjustments to SuSiE Code
The code below is a simplistic implementation of the above ideas.

(Note: Not all code is needed, some vestigial components from a copy-paste job from susieR)

```{r}
### NOTE: This is basic code, and I did not attempt to mirror the level of numerical sophistication in the susie functions
### If this idea is worth pursuing further, then this code can be improved

susie_logistic = function(Y, X, L = 10, V = 1, prior_weights = NULL, tol = 1e-3, maxit = 1000, intercept = TRUE) {
  
  p = ncol(X)
  n = nrow(X)
  
  # place to store posterior info for each l = 1, ..., L
  post_alpha = matrix(NA, nrow = p, ncol = L)
  post_mu = matrix(NA, nrow = p, ncol = L)
  post_sigma = matrix(NA, nrow = p, ncol = L)
  post_info = list(post_alpha = post_alpha, post_mu = post_mu, post_sigma = post_sigma)

  beta_post_init = matrix(Inf, nrow = p, ncol = L) # initialize
  beta_post = matrix(0, nrow = p, ncol = L)
  
  fixed = rep(0, n) # fixed portion, estimated from l' != l other SER models
  
  iter = 0
  while(norm(beta_post - beta_post_init, "1") > tol) { # repeat until posterior means converge (ELBO not calculated here, so use this convergence criterion instead)
    beta_post_init = beta_post
      
    for (l in 1:L) {
      
      # below is old (inefficient) calculation of the fixed portion
      #fixed = rowSums(X %*% beta_post[, -l]) + int.coef # fixed portion from previous estimates (add intercept portion as well)
      fixed = fixed - (X %*% beta_post[, l]) # remove effect from previous iteration
      
      SER_logistric_l = single_effect_regression_logistic(Y, X, V, prior_weights, FALSE, fixed, intercept)
      # store
      post_info$post_alpha[, l] = SER_logistric_l$alpha
      post_info$post_mu[, l] = SER_logistric_l$mu
      post_info$post_sigma[, l] = SER_logistric_l$mu2 - SER_logistric_l$mu^2
      
      # update beta_post
      beta_post[, l] = SER_logistric_l$alpha * SER_logistric_l$mu
      
      fixed = fixed + (X %*% beta_post[, l]) # add back new fixed portion
      
      iter = iter + 1
      if (iter > maxit) {
        stop("Maximum number of iterations reached")
      }
      
    }
    
  }
  
  return(post_info)
  
}


# SER_logistic function
single_effect_regression_logistic = function(Y, X, V, prior_weights = NULL, optimize_V = FALSE, fixed = NULL, intercept = TRUE) {
  p = ncol(X)
  
  betahat = numeric(p)
  shat2 = numeric(p)
  
  if (is.null(fixed)) { # fixed is components from previous SER fits
    fixed = rep(0, length(Y))
  }
  
  # NOTE: could parallelize loop below if desired
  for (j in 1:p) { # logistic regression on each column of X separately
    if (intercept) {
      log.fit = glm(Y ~ X[, j] + 1 + offset(fixed), family = "binomial") # fit w/ intercept
    } else {
      log.fit = glm(Y ~ X[, j] - 1 + offset(fixed), family = "binomial") # fit w/out intercept
    }
    log.fit.coef = summary(log.fit)$coefficients
    # NOTE: coerces "intercept" to be 0 or 1 to grab relevant row of glm coefficient output
    betahat[j] = ifelse(is.na(log.fit.coef[1 + intercept, 1]), 0, log.fit.coef[1 + intercept, 1]) # beta-hat MLE (if na, just set to 0)
    shat2[j] = ifelse(is.na(log.fit.coef[1 + intercept, 2]), Inf, log.fit.coef[1 + intercept, 2]^2) # (std errof beta-hat MLE)^2 (if na, just set to Inf)
  }
  
  if (is.null(prior_weights)) {
    prior_weights = rep(1 / p, p)
  }
  
  if(optimize_V) {
    stop("Optimizing for prior variance not yet implemented for logistic case")
    #if(loglik.grad(0, betahat, shat2, prior_weights) < 0) {
    #  V = 0
    #} else {
    ##V.o = optim(par=log(V),fn=negloglik.logscale,gr = negloglik.grad.logscale,betahat=betahat,shat2=shat2,prior_weights=prior_weights,method="BFGS")
    ##if(V.o$convergence!=0){
    ##  warning("optimization over prior variance failed to converge")
    ##}
    #  V.u = uniroot(negloglik.grad.logscale, c(-10, 10), extendInt = "upX", betahat = betahat, shat2 = shat2, prior_weights = prior_weights)
    #  V = exp(V.u$root)
    #}
  }
  
  lbf = dnorm(betahat, 0, sqrt(V + shat2), log = TRUE) - dnorm(betahat, 0, sqrt(shat2), log = TRUE)
  #log(bf) on each SNP
  
  lbf[is.infinite(shat2)] = 0 # deal with special case of infinite shat2 (eg happens if X does not vary)
  
  maxlbf = max(lbf)
  w = exp(lbf - maxlbf) # w is proportional to BF, but subtract max for numerical stability
  # posterior prob on each SNP
  w_weighted = w * prior_weights
  weighted_sum_w = sum(w_weighted)
  alpha = w_weighted / weighted_sum_w
  post_var = 1 / ((1 / shat2) + (1 / V)) # posterior variance
  post_mean = (1 / shat2) * post_var * betahat # posterior mean
  post_mean2 = post_var + post_mean^2 # posterior second moment
  # BF for single effect model
  lbf_model = maxlbf + log(weighted_sum_w)
  # NOTE: Need to double check below
  loglik = lbf_model + log(1/2)*length(Y) # loglik of 0/1 response Y under p = .5
  return(list(alpha = alpha, mu = post_mean, mu2 = post_mean2, lbf = lbf, lbf_model = lbf_model, V = V, loglik = loglik))
}
```


# Demonstration

## Easy Demonstration
### L = 1
As a very easy demonstration, let $n = 1000, p = 10, L = 1$, and let all columns of $X$ be orthonormal (obtained from the SVD of a random matrix). WLOG, we let the first element of $\mathbf{b}$ be the only non-zero element. We set $\sigma_0^2 = 25$.

We then simulated $\mathbf{y}$ from the specified bernoulli model,
$$
y_i \stackrel{\perp}{\sim} \text{Bern}\Bigg(\frac{e^{(\mathbf{Xb})_i}}{1 + e^{(\mathbf{Xb})_i}}\Bigg)
$$
(We do not add an intercept term here, but still fit the model with an intercept).

We repeat this procedure 100 times.

```{r}
set.seed(1138)

n = 1000
p = 10
L = 1
V = 25
beta_true = rep(0, p)

# make orthonormal covariates
rand_mat = matrix(rnorm(n * p), nrow = n)
svd_rand_mav = svd(rand_mat)
X = svd_rand_mav$u
# make response
# make response

B = 100
b_1s = numeric(B)
susie.fits.easy = list() # logistic SuSiE fits
logistic.fits.easy = list() # logistic regression fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
  susie.fits.easy[[i]] = susie_logistic(Y, X, L, V)
  logistic.fits.easy[[i]] = glm(Y ~ X, family = "binomial")
}
```

Figure 1 below plots the estimated value for $\hat{b}_1 := \mu_1 \cdot \alpha_1$ against the true value, where $\mu_1$ is the estimated posterior mean, and $\alpha_1$ is the estimated posterior inclusion probability. Figure 2 below plots the PIP, $\alpha_i$, against the true value of $b_1$.
```{r}
plot(sapply(susie.fits.easy, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Estimate", main = "Figure 1", pch = 19)
abline(0, 1)
abline(v = c(-5, 5))

plot(sapply(susie.fits.easy, function(x) x$post_alpha[1]) ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Inclusion Probability", main = "Figure 2", pch = 19)
abline(v = c(-5, 5))
```
Figure 1 shows that, in this simulation, we observe a shrinkage behavior similar to hard-thresholding. Figure 2 shows a sharp transition between a very low PIP and a PIP near 1. This sharp transition is what causes the hard-thresholding-like shrinkage we obsersve: the posterior mean values $\mu_1$ closely match the true values for $b_1$, but are effectively shrunk to 0 in this region by the PIP $\alpha_i$, and are effectively untouched outside this region.

We can compare figure 2 with the p-values obtained from the normal logistic regressions on the same simulated data, shown in figure 3 below.
```{r}
plot(sapply(logistic.fits.easy, function(x) summary(x)$coefficients[2, 4]) ~ b_1s, xlab = "True Effect Value", ylab = "Logistic p-value", main = "Figure 3", pch = 19)
abline(v = c(-5, 5))
```
We see a similar phase transition around the same values of the true effect size, $|b_1| \le 5$.

### L = 2
We not repeat the same as above, except with $L = 2$, and the second element $b_2$ also a non-zero.

```{r, cache = T}
set.seed(1138)

n = 1000
p = 10
L = 2
V = 25
beta_true = rep(0, p)

# make orthonormal covariates
rand_mat = matrix(rnorm(n * p), nrow = n)
svd_rand_mav = svd(rand_mat)
X = svd_rand_mav$u
# make response
# make response

B = 100
b_1s = numeric(B)
b_2s = numeric(B)
susie.fits.easy2 = list() # logistic SuSiE fits
logistic.fits.easy2 = list() # logistic regression fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  beta_true[2] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  b_2s[i] = beta_true[2]
  Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
  susie.fits.easy2[[i]] = susie_logistic(Y, X, L, V)
  logistic.fits.easy2[[i]] = glm(Y ~ X, family = "binomial")
}
```


Figures 4(a-b) below plot the estimated value for $\hat{b}_1 := \mu_1 \cdot \alpha_1$ and $\hat{b}_2 := \mu_2 \cdot \alpha_2$ against the true value, where $\mu_i$ is the estimated posterior mean, and $\alpha_i$ is the estimated posterior inclusion probability. Figures 5(a-b) below plot the PIP, $\alpha_i$, against the true value of $b_i$.

(Note: In order to determine which estimated effect vector corresponds to true effect 1 and true effect 2, I have chosen the estimated effect vector that maximizes the PIP for true effect 1/2. This is not perfect, but is an easy way to divide up the 100 replications)
```{r}
est_b1_max = sapply(susie.fits.easy2, function(x) x$post_alpha[1, which.max(x$post_alpha[1, ]) ]* x$post_mu[1, which.max(x$post_alpha[1, ])])

est_b2_max = sapply(susie.fits.easy2, function(x) x$post_alpha[2, which.max(x$post_alpha[2, ]) ]* x$post_mu[2, which.max(x$post_alpha[2, ])])

est_alpha1_max = sapply(susie.fits.easy2, function(x) max(x$post_alpha[1, ]))

est_alpha2_max = sapply(susie.fits.easy2, function(x) max(x$post_alpha[2, ]))

plot(est_b1_max ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Estimate", main = "Figure 4(a) - Entry 1", pch = 19)
abline(0, 1)
abline(v = c(-5, 5))

plot(est_b2_max ~ b_2s, xlab = "True Effect Value", ylab = "Posterior Estimate", main = "Figure 4(b) - Entry 2", pch = 19)
abline(0, 1)
abline(v = c(-5, 5))

plot(est_alpha1_max ~ b_1s, xlab = "True Effect Value", ylab = "Posterior Inclusion Probability", main = "Figure 5(a) - Entry 1", pch = 19)
abline(v = c(-5, 5))

plot(est_alpha2_max ~ b_2s, xlab = "True Effect Value", ylab = "Posterior Inclusion Probability", main = "Figure 5(b) - Entry 2", pch = 19)
abline(v = c(-5, 5))
```
We see a similar pattern as above, in the case $L = 1$. The method is also able to detect both effects (when strong enough).

We can compare figures 5(a-b) with the p-values obtained from the normal logistic regressions on the same simulated data, shown in figures 6(a-b) below.
```{r}
plot(sapply(logistic.fits.easy2, function(x) summary(x)$coefficients[2, 4]) ~ b_1s, xlab = "True Effect Value", ylab = "Logistic p-value", main = "Figure 6(a) - Entry 1", pch = 19)
abline(v = c(-5, 5))

plot(sapply(logistic.fits.easy2, function(x) summary(x)$coefficients[3, 4]) ~ b_2s, xlab = "True Effect Value", ylab = "Logistic p-value", main = "Figure 6(b) - Entry 2", pch = 19)
abline(v = c(-5, 5))
```
We see a similar phase transition around the same values of the true effect size, $|b_i| \le 5$.

## Medium Demonstration
As a medium difficulty demonstration, let $n = 100, p = 10, L = 3, \sigma_0^2 = 5$.

We create the data $\mathbf{X}$ where all entries are iid standard normal. I then set the 2nd column to be identical to the 3rd, and the 7th column identical to the 6th (when running this simulation, $b_3$ and $b_6$ were generated to be non-zero, and $b_2$ and $b_7$ were generated to be 0, so this was done in the spirit of the toy example from the paper).

We then simulated $\mathbf{y}$ from the specified bernoulli model,
$$
y_i \stackrel{\perp}{\sim} \text{Bern}\Bigg(\frac{e^{(\mathbf{Xb})_i}}{1 + e^{(\mathbf{Xb})_i}}\Bigg)
$$

```{r}
### TEST susie_logistic
set.seed(1138)

n = 100
p = 10
L = 3
pi = rep(1 / p, p) # prior weights
V = 5 # prior variance

beta_true = rep(0, p)
for (l in 1:L) {
  b_l = rnorm(1, 0, sqrt(V))
  gamma_l = rmultinom(1, 1, pi)
  beta_l = b_l * gamma_l
  beta_true = beta_true + beta_l
}

# simulate data, induce correlations
X = matrix(rnorm(n*p, 0, 1), nrow = n, ncol = p)
X[, 2] = X[, 3]
X[, 7] = X[, 6]

# make response
Y = rbinom(n, 1, exp(X %*% beta_true + 2) / (1 + exp(X %*% beta_true + 2)))

susie.logistic.fit = susie_logistic(Y, X, L, V)
```

Figure 7 below plots the true values for $\mathbf{b}$.
```{r}
plot(beta_true, ylab = "True Value", main = "Figure 7", pch = 19)
```
The 3rd and 6th are around -1.3, and the 5th is around 0.75.

Figures 8(a-c) below plots the posterior means for the 3 vectors estimated from this procedure (calculated as $\mathbf{\bar{b}}_l := \alpha_l \circ \mu_{1l}$ using $\alpha_l, \mu_{1l}$ returned from the logistic-version of the IBFS algorithm):
```{r}
b_1_post = susie.logistic.fit$post_mu[, 1] * susie.logistic.fit$post_alpha[, 1]
b_2_post = susie.logistic.fit$post_mu[, 2] * susie.logistic.fit$post_alpha[, 2]
b_3_post = susie.logistic.fit$post_mu[, 3] * susie.logistic.fit$post_alpha[, 3]

plot(b_1_post, ylab = "Posterior Mean Value", main = "Figure 8(a)", pch = 19)
plot(b_2_post, ylab = "Posterior Mean Value", main = "Figure 8(b)", pch = 19)
plot(b_3_post, ylab = "Posterior Mean Value", main = "Figure 8(c)", pch = 19)
```
We can see that the first estimated vector captures our constructed correlation between the 2nd and 3rd columns, the second estimated vector captures our constructed correlation between the 6th and 7th columns, and the third estimated vector captures the individual effect of the 5th column.

Figures 9(a-c) below plots our estimated PIPs, $\alpha_1, \alpha_2, \alpha_3$:
```{r}
plot(susie.logistic.fit$post_alpha[, 1], ylab = "Posterior Inclusion Probability", main = "Figure 9(a)", pch = 19)
plot(susie.logistic.fit$post_alpha[, 2], ylab = "Posterior Inclusion Probability", main = "Figure 9(b)", pch = 19)
plot(susie.logistic.fit$post_alpha[, 3], ylab = "Posterior Inclusion Probability", main = "Figure 9(c)", pch = 19)
```
These groups of correlated predictors are shows from the PIPs.


## Hard Demonstration
As a hard demonstration, let $n = 1,000, p = 100, L = 10, \sigma_0^2 = 5$.

We create the data $\mathbf{X}$ where all entries are iid standard normal. I then set the 2nd column to be identical to the 1st, the 88th column identical to the 87th, and the 89th column highly negatively correlated with the 87th (when running this simulation, $b_1$ and $b_87$ were generated to be non-zero, and $b_2$, $b_{88}$ and $b_{89}$ were generated to be 0, so this was done in the spirit of the toy example from the paper).

We then simulated $\mathbf{y}$ from the specified bernoulli model,
$$
y_i \stackrel{\perp}{\sim} \text{Bern}\Bigg(\frac{e^{(\mathbf{Xb})_i}}{1 + e^{(\mathbf{Xb})_i}}\Bigg)
$$

```{r, cache = T}
set.seed(1138)

n = 1000
p = 100
L = 10
pi = rep(1 / p, p) # prior weights
V = 5 # prior variance

beta_true = rep(0, p)
for (l in 1:L) {
  b_l = rnorm(1, 0, sqrt(V))
  gamma_l = rmultinom(1, 1, pi)
  beta_l = b_l * gamma_l
  beta_true = beta_true + beta_l
}

# simulate data, induce correlations
X = matrix(rnorm(n*p, 0, 1), nrow = n, ncol = p)
X[, 2] = X[, 1]
X[, 88] = X[, 87]
X[, 89] = runif(n, -1, -.7) * X[, 87]

# make response
Y = rbinom(n, 1, exp(X %*% beta_true + 2) / (1 + exp(X %*% beta_true + 2)))

susie.logistic.fit = susie_logistic(Y, X, L, V)
```

Figure 10 below plots the true values for $\mathbf{b}$ (the numbers correspond to the points and indices of non-zero true effects).
```{r}
plot(beta_true[beta_true == 0] ~ which(beta_true == 0), xlim = c(0, 101), ylim = range(beta_true) + c(-.1, .1), xlab = "Index", ylab = "True Value", main = "Figure 10", pch = 19)
text(which(beta_true != 0), beta_true[beta_true != 0], labels = which(beta_true != 0))
```
We can see that true effects 41 and 56 are very small, and 27, 60, and 87 are also small-ish.

Figures 11(a-j) below plots the posterior means for the 10 vectors estimated from this procedure (calculated as $\mathbf{\bar{b}}_l := \alpha_l \circ \mu_{1l}$ using $\alpha_l, \mu_{1l}$ returned from the logistic-version of the IBFS algorithm):
```{r, fig.height = 12}
par(mfrow = c(5, 2))
for (l in 1:L) {
  b_l_post = susie.logistic.fit$post_mu[, l] * susie.logistic.fit$post_alpha[, l]
  plot(b_l_post[abs(b_l_post) < .01] ~ which(abs(b_l_post) < .01), xlim = c(0, 101), ylim = range(b_l_post) + c(-.1, .1), xlab = "Index", ylab = "Posterior Mean Value", main = paste("Figure 11(", letters[l], ")", sep = ""), pch = 19)
  text(which(abs(b_l_post) >= .01), b_l_post[abs(b_l_post) >= .01], labels = which(abs(b_l_post) >= .01))
}
par(mfrow = c(1, 1))
```
(NOTE: In figures 11(a) and 11(h), 1 and 2 are close together, so it looks like the number 12. But it is really just 1 and 2).

We can see that the first estimated vector (11a) (and 8th, 11(h)) captures our constructed correlation between the 1st and 2nd columns, and 7th vector (11g) captures our constructed correlation between the 87th, 88th, and 89th columns (note the signs: 89 was constructed to be negatively correlated with the true effect column 87).

We also see that the groups (1, 2) and (76) are each captured twice, lending to their large coefficients. As a result, we have not captured the true (small) effects from 41 and 56.

Figures 12(a-j) below plots our estimated PIPs, $\alpha_1, \alpha_2, \alpha_3$:
```{r, fig.height = 12}
par(mfrow = c(5, 2))
for (l in 1:L) {
  alpha_l = susie.logistic.fit$post_alpha[, l]
  plot(alpha_l[alpha_l < .01] ~ which(alpha_l < .01), xlim = c(0, 101), ylim = c(0, 1), xlab = "Index", ylab = "Posterior Inclusion Probability", main = paste("Figure 12(", letters[l], ")", sep = ""), pch = 19)
  text(which(alpha_l >= .01), alpha_l[alpha_l >= .01], labels = which(alpha_l >= .01))
}
par(mfrow = c(1, 1))
```
These groups of correlated predictors are shows from the PIPs.


## Signal Level Testing
In this section, I keep the same model matrix $X$, but center and scale the columns to have unit 2-norm. I then set a single non-zero effect with a range of signal levels ($\sigma_0^2$) to see at what levels the signal can be found. I also set another column of X to be highly positively correlated with the true effect column.

I have set $n = 1000, p = 10, L = 1$.

Here, I always set the first element to be non-zero, and the 5th column to be correlated with the 1st.

At each signal level $\sigma_0^2 \in \{1, 5, 10, 25, 50\}$, I replicate $B = 10$ times the following procedure:

1. Draw $b_1 \sim \mathcal{N}(0, \sigma_0^2)$;

2. Simulate $Y_i \sim \text{Bern}\Bigg(\frac{e^{x_i^T b}}{1 + e^{x_i^T b}}\Bigg)$;

3. Run the logistic version of SuSiE with an intercept on the data with the noise level fixed and known.

In the plots below, the first set of 5 show the posterior mean values, $\mu_{1} \circ \alpha$. For indices where the PIP, $\alpha_j$, was $\ge 0.05$, I show the point as its index number (ideally, we would only see 1 and 5). Otherwise, the point is a dot.

The second set of 5 show the PIPs, again with a number if the PIP was $\ge 0.05$.

```{r, fig.height = 12}
set.seed(1138)
B = 10 # times to repeat at each noise level

n = 1000
p = 10
L = 1

X = matrix(rnorm(n*p, 0, 1), nrow = n, ncol = p)
X[, 5] = runif(n, .7, 1) * X[, 1]

X = apply(X, MARGIN = 2, function(x) x - mean(x))
X = apply(X, MARGIN = 2, function(x) x / norm(x, "2"))

beta_true = rep(0, p)

Vs = c(1, 5, 10, 25, 50)
beta_true_1 = matrix(nrow = length(Vs), ncol = B)
susie.logistic.list = list()
for (i in 1:length(Vs)) {
  V = Vs[i]
  susie.logistic.list[[i]] = list()
  names(susie.logistic.list)[i] = paste("Vs_", V, sep = "")
  for (j in 1:B) {
    beta_true[1] = rnorm(1, 0, sqrt(V))
    Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
    susie.logistic.list[[i]][[j]] = susie_logistic(Y, X, L, V)
    beta_true_1[i, j] = beta_true[1]
  }
}

par(mfrow = c(5, 2))
for (i in 1:length(Vs)) {
  V = Vs[i]
  for (j in 1:B) {
    a_l_post = susie.logistic.list[[i]][[j]]$post_alpha[, 1]
    b_l_post = susie.logistic.list[[i]][[j]]$post_mu[, 1] * a_l_post
    plot(b_l_post[abs(a_l_post) < .05] ~ which(abs(a_l_post) < .05), xlim = c(1, 10), ylim = range(b_l_post) + c(-.1, .1), xlab = "Index", ylab = "Posterior Mean Value", main = paste("True effect 1: ", round(beta_true_1[i, j], 2), " ::: Signal Level: ", round(V, 2), sep = ""), pch = 19)
    text(which(abs(a_l_post) >= .05), b_l_post[abs(a_l_post) >= .05], labels = which(abs(a_l_post) >= .05))
  }
}
par(mfrow = c(1, 1))


par(mfrow = c(5, 2))
for (i in 1:length(Vs)) {
  V = Vs[i]
  for (j in 1:B) {
    a_l_post = susie.logistic.list[[i]][[j]]$post_alpha[, 1]
    plot(a_l_post[abs(a_l_post) < .05] ~ which(abs(a_l_post) < .05), xlim = c(1, 10), ylim = c(0, 1), xlab = "Index", ylab = "Posterior Inclusion Probability", main = paste("True effect 1: ", round(beta_true_1[i, j], 2), " ::: Signal Level: ", round(V, 2), sep = ""), pch = 19)
    text(which(abs(a_l_post) >= .05), a_l_post[abs(a_l_post) >= .05], labels = which(abs(a_l_post) >= .05))
  }
}
par(mfrow = c(1, 1))
```

As we can see, we require relatively large effects in order to determine that 1 and/or 5 is contributing to the response. However, a regular logistic regression on the columns of X (removing the 5th column) in these settings also have a difficult time determing that the 1 is the effect variable.


## Signal Testing with Genotype Matrix
Here, I perform a similar test as above, except the design matrix has columns whose entries are $X_{ij} \in \{0, 1, 2\}$. I do not center or scale these variables. As before, the 1st effect is always the only non-null effect. And the 5th column is highly positively correlated with the 1st (w.p. 0.9, it copies column 1, otherwise it is random).

Here, we test $\sigma_0^2 \in \{1, 2, 3, 4, 5\}$

```{r, fig.height = 12}
set.seed(1138)
B = 10 # times to repeat at each noise level

n = 1000
p = 10
L = 1

X = matrix(rbinom(n * p, 2, runif(n * p)), nrow = n, ncol = p)
copy_ind = runif(n, 0, 1) <= 0.9
X[copy_ind, 5] = X[copy_ind, 1]

beta_true = rep(0, p)

Vs = 1:5
beta_true_1 = matrix(nrow = length(Vs), ncol = B)
susie.logistic.list = list()
for (i in 1:length(Vs)) {
  V = Vs[i]
  susie.logistic.list[[i]] = list()
  names(susie.logistic.list)[i] = paste("Vs_", V, sep = "")
  for (j in 1:B) {
    beta_true[1] = rnorm(1, 0, sqrt(V))
    Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
    susie.logistic.list[[i]][[j]] = susie_logistic(Y, X, L, V)
    beta_true_1[i, j] = beta_true[1]
  }
}

par(mfrow = c(5, 2))
for (i in 1:length(Vs)) {
  V = Vs[i]
  for (j in 1:B) {
    a_l_post = susie.logistic.list[[i]][[j]]$post_alpha[, 1]
    b_l_post = susie.logistic.list[[i]][[j]]$post_mu[, 1] * a_l_post
    plot(b_l_post[abs(a_l_post) < .05] ~ which(abs(a_l_post) < .05), xlim = c(1, 10), ylim = range(b_l_post) + c(-.1, .1), xlab = "Index", ylab = "Posterior Mean Value", main = paste("True effect 1: ", round(beta_true_1[i, j], 2), " ::: Signal Level: ", round(V, 2), sep = ""), pch = 19)
    text(which(abs(a_l_post) >= .05), b_l_post[abs(a_l_post) >= .05], labels = which(abs(a_l_post) >= .05))
  }
}
par(mfrow = c(1, 1))


par(mfrow = c(5, 2))
for (i in 1:length(Vs)) {
  V = Vs[i]
  for (j in 1:B) {
    a_l_post = susie.logistic.list[[i]][[j]]$post_alpha[, 1]
    plot(a_l_post[abs(a_l_post) < .05] ~ which(abs(a_l_post) < .05), xlim = c(1, 10), ylim = c(0, 1), xlab = "Index", ylab = "Posterior Inclusion Probability", main = paste("True effect 1: ", round(beta_true_1[i, j], 2), " ::: Signal Level: ", round(V, 2), sep = ""), pch = 19)
    text(which(abs(a_l_post) >= .05), a_l_post[abs(a_l_post) >= .05], labels = which(abs(a_l_post) >= .05))
  }
}
par(mfrow = c(1, 1))
```

We can see that much smaller effects are detectable in this setting than above, with an arbitrary center and scaled design matrix $X$.
