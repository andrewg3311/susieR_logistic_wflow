<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Andrew Goldstein" />

<meta name="date" content="2019-05-08" />

<title>susieR Logistic Regression VB</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">susieR_Logistic_wflow</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="susie_logistic_demonstration.html">Logistic GLM SuSiE</a>
</li>
<li>
  <a href="VB_susie_logistic_demonstration.html">Logistic VB SuSiE</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/jdblischak/workflowr">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">susieR Logistic Regression VB</h1>
<h4 class="author">Andrew Goldstein</h4>
<h4 class="date">May 8, 2019</h4>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span> workflowr <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> </a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2019-07-02
</p>
<p>
<strong>Checks:</strong> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 6 <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> 1
</p>
<p>
<strong>Knit directory:</strong> <code>susieR_Logistic_wflow/</code> <span class="glyphicon glyphicon-question-sign" aria-hidden="true" title="This is the local directory in which the code in this file was executed."> </span>
</p>
<p>
This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a> analysis was created with <a
  href="https://github.com/jdblischak/workflowr">workflowr</a> (version 1.4.0). The <em>Checks</em> tab describes the reproducibility checks that were applied when the results were created. The <em>Past versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>R Markdown file:</strong> up-to-date </a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate" class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git repository, you know the exact version of the code that produced these results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20181203code"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Seed:</strong> <code>set.seed(20181203)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20181203code" class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20181203)</code> was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Session information:</strong> recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongdetected"> <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> <strong>Cache:</strong> detected </a>
</p>
</div>
<div id="strongCachestrongdetected" class="panel-collapse collapse">
<div class="panel-body">
The following chunks had caches available:
<ul>
<li>
unnamed-chunk-13
</li>
<li>
unnamed-chunk-14
</li>
<li>
unnamed-chunk-19
</li>
</ul>
<p>To ensure reproducibility of the results, delete the cache directory <code>VB_susie_logistic_demonstration_cache</code> and re-run the analysis. To have workflowr automatically delete the cache directory prior to building the file, set <code>delete_cache = TRUE</code> when running <code>wflow_build()</code> or <code>wflow_publish()</code>.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>File paths:</strong> relative </a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr project makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomandrewg3311susieRlogisticwflowtree95524525954d87b90796fd84071cb88426dd940ftargetblank9552452a"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Repository version:</strong> <a href="https://github.com/andrewg3311/susieR_logistic_wflow/tree/95524525954d87b90796fd84071cb88426dd940f" target="_blank">9552452</a> </a>
</p>
</div>
<div id="strongRepositoryversionstrongahrefhttpsgithubcomandrewg3311susieRlogisticwflowtree95524525954d87b90796fd84071cb88426dd940ftargetblank9552452a" class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility. The version displayed above was the version of the Git repository at the time these results were generated. <br><br> Note that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .Rhistory
    Ignored:    analysis/.Rhistory
    Ignored:    analysis/VB_susie_logistic_demonstration_cache/
    Ignored:    analysis/gene_enrichment_example_cache/
    Ignored:    analysis/susie_logistic_demonstration_cache/

Unstaged changes:
    Deleted:    code/logistic_susie_functions.R

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the R Markdown and HTML files. If you’ve configured a remote Git repository (see <code>?wflow_git_remote</code>), click on the hyperlinks in the table below to view them.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/95524525954d87b90796fd84071cb88426dd940f/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">9552452</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
<td>
Adding section for estimating prior variance. Still need to add simulations
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/9c8142eabaa34b3b78afbe700e96c64f2218048f/docs/VB_susie_logistic_demonstration.html" target="_blank">9c8142e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/e6926599c75fbd2c270d1d191dd46bed76d70af4/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">e692659</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
<td>
Adding autodep = T for cache
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/VB_susie_logistic_demonstration.html" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/3d895c9b4f90c2e9978008dd5fcecdb5e97da7ad/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">3d895c9</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
<td>
Updating the VB demo with new code, new math, and testing w/ covariates
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/9ef3d12a1aed0b5d4aca00b1adef91554472bc1d/docs/VB_susie_logistic_demonstration.html" target="_blank">9ef3d12</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-09
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/4941b38537d5f20772578682e720dec7107bebda/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">4941b38</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-09
</td>
<td>
Adding another comparison showing issue with GLM algo with separable data
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/40ae4d48747742ebc5d09188526857043bba8e40/docs/VB_susie_logistic_demonstration.html" target="_blank">40ae4d4</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fb18fab01f6032e8064bb287ebdd9bc7b02459e/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">6fb18fa</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
<td>
Minor formatting changes, and adding plot to comparison L=1
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/68bcd880028c037b3359b879ded235ee9c6ff6b5/docs/VB_susie_logistic_demonstration.html" target="_blank">68bcd88</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/5e270b116374e947dcba4206cf3c98690d604dec/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">5e270b1</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
<td>
Adding notes on runtime, WILL PERFORM TIME TRIALS LATER
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/VB_susie_logistic_demonstration.html" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/90ada57d371b174cbb3e20c6891fd4c966103ea1/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">90ada57</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
<td>
Updated algebraic comparison, moving towards top of the page
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/b863a95a633077c588fdfd67e12ad92a13cead95/docs/VB_susie_logistic_demonstration.html" target="_blank">b863a95</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/ab040d12878903c9ad45c15e2b0a2c9c64299120/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">ab040d1</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Minor tweak to get math to render properly at the end (still not finished, though)
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/1b300874c407aa48f76c2ea867cb6a20a0066fe5/docs/VB_susie_logistic_demonstration.html" target="_blank">1b30087</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/bc021e306c8aeb93db6ba31979e085016c9239e6/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">bc021e3</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Reverting back from previous change, I was right the first time (oops!)
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/VB_susie_logistic_demonstration.html" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/8aed9514f27c5cf8404b741e71131e46922d9a8c/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">8aed951</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Fixing another error in PIP math and implementation (error in completing the square)
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/5c41e85c0cb729173b651c9b0751c2a567e12ae3/docs/VB_susie_logistic_demonstration.html" target="_blank">5c41e85</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/3aad3a577662812f494a5c069bfe75afd3850efc/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">3aad3a5</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Fixing error in PIP math and implementation. Adding some more comparisons w/ GLM version
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/2802739d2a3e9df8afffda8259f7cfcb8417ab2d/docs/VB_susie_logistic_demonstration.html" target="_blank">2802739</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/ff7f3ecc2af246d1e57fef08a9145aaea464b54d/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">ff7f3ec</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Updating code for numerical stability for PIPs (got lost somehow w/ a copy-paste job)
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/683bf1b3a0daf550b9f2ef502dabf4b1542ac2cd/docs/VB_susie_logistic_demonstration.html" target="_blank">683bf1b</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/445a11b9455779fca296380c07ac5300b118a464/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">445a11b</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Fixing spacing on some Math
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/9fa2301ad69c033faef97129b5abb99a185eb653/docs/VB_susie_logistic_demonstration.html" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/b3459c9283e48442d03b89fc02966f4d0f6879d1/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">b3459c9</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
<td>
Updating w/ first comparisons with GLM logistic SuSiE
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/acba6fcd7a27dc3fadefb8369f0d96276ae4a67c/docs/VB_susie_logistic_demonstration.html" target="_blank">acba6fc</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/5ea56ea536904a99cbcc7263977e4bea2985061a/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">5ea56ea</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
<td>
Changing output format so old tests works
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/andrewg3311/susieR_logistic_wflow/c62537b6e3875acc75330707685f6af94ddf16a2/docs/VB_susie_logistic_demonstration.html" target="_blank">c62537b</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/b3145350e6d073ee0fdf527e21ce026de0d7c021/analysis/VB_susie_logistic_demonstration.Rmd" target="_blank">b314535</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
<td>
Adding VB version
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This page aims to explore an analog to SuSiE applied to 0/1 data using logistic regression. In my first attempt, I tried to replace the Single Effect Regression (SER) step in the Iterative Bayesian Forward Selection (IBFS) algorithm with an analogous logistic regression step. See <a href="susie_logistic_demonstration.html">here</a>. Here, instead of relying on the linear SuSiE results and modifying the updates to what the analogous updates seem like they should be in the logistic case, I start from the ground up and derive the variational updates in this case directly.</p>
<p>Our response is <span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span>, and our covariates are <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span>.</p>
<div id="full-model" class="section level2">
<h2>Full Model</h2>
<p>The full SuSiE model is as follows: <span class="math display">\[
\begin{aligned}
\mathbf{y} \sim \text{Bern}\Bigg(\frac{e^\mathbf{Xb}}{1 + e^{\mathbf{Xb}}}\Bigg) \quad \text{(element-wise)} \\
\mathbf{b} = \sum_{l = 1}^L \mathbf{b}_l \\
\mathbf{b}_l = \gamma_l b_l \quad (\text{independently for } l = 1, \dots, L) \\
\gamma_l \sim \text{Mult}(1, \mathbf{\pi}) \\
b_l \sim \mathcal{N}(0, \sigma_{0l}^2)
\end{aligned}
\]</span></p>
</div>
<div id="variational-algorithm" class="section level2">
<h2>Variational Algorithm</h2>
<p>As in linear SuSiE, we optimize over the class of distributions that factorize over <span class="math inline">\(b_l\)</span>, i.e. <span class="math inline">\(Q(\mathbf{b}) = \prod_{l=1}^L q_l(\mathbf{b}_l)\)</span>. Normally, the updates would be <span class="math inline">\(q_l(\mathbf{b}_l) \propto \exp\{\mathbb{E}_{-(q_l)}[\log (p(y|\mathbf{b}_1, \dots, \mathbf{b}_L) \cdot p(\mathbf{b}_1, \dots, \mathbf{b}_L)\}\)</span>. However, this gets ugly quickly, in large part due to the <span class="math inline">\(\log(1 + \exp\{X\mathbf{b}\})\)</span> term in the likelihood.</p>
<p>Instead, we can use a lower bound, <span class="math inline">\(h\)</span> on the likelihood <span class="math inline">\(p(y|\mathbf{b}) \ge h(\mathbf{b}; \xi)\)</span> in our algorithm (<span class="math inline">\(\xi\)</span> are variational parameters that we optimize over). So the derivation of VB is <span class="math display">\[
\log(p(y)) = \log \Big(\int p(y|b) p(b) db\Big) \ge \log \Big(\int h(b;\xi) p(b) db\Big) \ge [\text{Jensen}] \ge \int q(b) \log \frac{h(b;\xi)p(b)}{q(b)} db
\]</span> Proceeding in the same manner as the general derivation of VB, we obtain the following updates: <span class="math display">\[
\begin{aligned}
q_l(\mathbf{b}_l) \propto \exp\{\mathbb{E}_{-(q_l)}[\log (h(\mathbf{b}; \xi) \cdot p(\mathbf{b}_l)\} \\
\xi = \arg \max_\xi \mathbb{E}_{\mathbf{b} \sim Q}[\log h(\mathbf{b};\xi)]
\end{aligned}
\]</span></p>
<p>For our lower bound, we turn to <a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.210">“A Variational Approach to Bayesian Logistic Regression Models and their Extensions” (Jaakkola and Jordan, 1996)</a>. In this paper, they state the following lower bound result in the logistic response setting:</p>
<p>Let <span class="math inline">\(g(x) = \frac{e^x}{1 + e^x}\)</span> (the inverse logit transformation). Then <span class="math display">\[
P(y_i|x_i, \beta) = g(X_y) \ge g(\xi) \exp\Big\{\frac{X_y - \xi}{2} + \lambda(\xi)(X_y^2 - \xi^2)\Big\}
\]</span> where <span class="math inline">\(X_y = (2y_i - 1)x_i^T \beta\)</span> and <span class="math inline">\(\lambda(\xi) = \frac{1}{2\xi} (g(\xi) - \frac{1}{2})\)</span>. (The same bound is used in <a href="http://approximateinference.org/2017/accepted/Horri2017.pdf">“Sparse Bayesian Logistic Regression with Hierarchical Prior and Variational Inference” (Horii, 2017)</a>.</p>
<p>Using this bound, we derive the VB updates as follows: <span class="math display">\[
\begin{aligned}
q_l(b_l = c \cdot e_j) \propto \exp\Bigg\{\log p(b_l = c e_j) + \sum_{i=1}^n \Big[\log g(\xi_i) + y_i x_i^T(ce_j + \sum_{k \ne l} \bar{b_k} - \\
\frac{x_i^T(ce_j + \sum_{k \ne l} \bar{b_k}) + \xi_i}{2} - \frac{1}{2\xi_i}\Big(g(\xi_i) - \frac{1}{2}\Big)\Big((\mathbb{E}_{-(q_l)}[(x_i^T\sum_{k=1}^L b_k)^2] - \xi_i^2\Big)\Big]\Bigg\} \\
\text{Note: } \mathbb{E}_{-(q_l)}[(x_i^T\sum_{k=1}^L b_k)^2]  = \mathbb{E}_{-(q_l)}[(c x_i^Te_j + \sum_{k \ne l}x_i^Tb_k)^2] = \\
c^2 (x_i^Te_j)^2 + x_i^T \mathbb{E}_{-(q_l)}[(\sum_{k \ne l}b_k)^2] + 2(cx_i^Te_j)(x_i^T\sum_{k \ne l}\bar{b_k}) = c^2 (x_i^Te_j)^2 + 2(cx_i^Te_j)(x_i^T\sum_{k \ne l}\bar{b_k}) + (const \; in \; b_l) \\
\therefore q_l(b_l = c \cdot e_j) \propto \exp\Bigg\{\log p(b_l = c e_j) + \sum_{i=1}^n \Big[\log g(\xi_i) + y_i x_i^T(ce_j + \sum_{k \ne l} \bar{b_k} - \frac{x_i^T(ce_j + \sum_{k \ne l} \bar{b_k}) + \xi_i}{2} - \\
\frac{1}{2\xi_i}\Big(g(\xi_i) - \frac{1}{2}\Big)\Big((c^2(x_i^Te_j)^2 + (cx_i^Te_j)(x_i^T \sum_{k \ne l} \bar{b_k}) - \xi_i^2\Big)\Big]\Bigg\} \propto \\
[\text{collect terms and complete the square}] \\
\propto \exp\Bigg\{\log(\pi_j) - \frac{\tau_{jl}}{2}\Bigg(c - \frac{\nu_{jl}}{\tau_{jl}}\Bigg)^2 + \frac{\nu_{jl}^2}{2\tau_{jl}} \pm \frac12 \log(1 / \tau_{jl})\Bigg\} \\
\text{where } \nu_{jl} = \sum_{i=1}^n x_i^Te_j\Big(y_i - \frac{1}{2} - \frac{1}{\xi_i}(g(\xi_i) - \frac{1}{2})(x_i^T \sum_{k \ne l}\bar{b_k})\Big) \text{ and } \tau_{jl} = \frac{1}{\sigma_0^2} + \sum_{i=1}^n \frac{1}{\xi_i}(g(\xi_i) - \frac{1}{2})(x_i^Te_j)^2
\end{aligned}
\]</span> Note that this is a mixture of a multinoulli draw for the non-zero element of <span class="math inline">\(b_l\)</span> and then a normal draw for the value of that non-zero element. In particular, <span class="math display">\[
\begin{aligned}
q(\gamma_l = j) \propto \pi_j \cdot \sqrt{\frac{1}{\tau_{jl}}} \cdot \exp \Bigg\{\frac{\nu_{jl}^2}{2\tau_{jl}}\Bigg\} \\
b_{l_j}|\gamma_l = j \sim_{q_l} \mathcal{N}\Big(\frac{\nu_{jl}}{\tau_{jl}}, \frac{1}{\tau_{jl}}\Big)
\end{aligned}
\]</span></p>
<p>For the updates to <span class="math inline">\(\xi\)</span>: <span class="math display">\[
\begin{aligned}
\xi = \arg \max_\xi \mathbb{E}_{b \sim Q}[\log h(\beta; \xi)] = \arg \max_\xi \sum_{i=1}^n \log(g(\xi_i)) + y_i x_i^T\mathbb{E}_{b \sim Q}[\mathbf{b}] - \frac{x_i^T\mathbb{E}_{b \sim Q}[\mathbf{b}] + \xi_i}{2} - \frac{1}{2 \xi_i}(g(\xi_i) - \frac{1}{2})(\mathbb{E}_{b \sim Q}[(x_i^T\mathbf{b})^2] - \xi_i^2) = \\
[\text{remove constants in } \xi_i] = \arg \max_\xi \sum_{i=1}^n \log(g(\xi_i))- \frac{\xi_i}{2} - \frac{1}{2 \xi_i}(g(\xi_i) - \frac{1}{2})(\mathbb{E}_{b \sim Q}[(x_i^T\mathbf{b})^2] - \xi_i^2)
\end{aligned}
\]</span> We maximize over the expected log-“likelihood” (really a lower-bound on the likelihood) since this is equivalent to maximizing the ELBO with respect to <span class="math inline">\(\xi\)</span> (since the KL divergence does not depend of <span class="math inline">\(\xi\)</span>).</p>
<p>Note that this is separable in <span class="math inline">\(\xi_i\)</span>, so we can optimize over each <span class="math inline">\(\xi_i\)</span> separately. Note further that <span class="math display">\[
\begin{aligned}
Q_i := \mathbb{E}_{b \sim Q}[(x_i^T\mathbf{b})^2] = \mathbb{E}_{b_1 \sim q_1, \dots, b_L \sim q_L}[(x_i^T \sum_{l=1}^L b_l)^2] = \mathbb{E}[\sum_{l=1}^L(x_i^T b_l)^2 + 2\sum_{l=1}^L \sum_{k &gt; l}^L (x_i^T b_l)(x_i^T b_k)] = \\
[\text{mean field approximation, so } b_l \perp b_k] = \sum_{l=1}^L \mathbb{E}_{q_l}[(x_i^Tb_l)^2] + 2 \sum_{l=1}^L \sum_{k &gt; l}^L (x_i^T \bar{b_l})(x_i^T \bar{b_k}) \\
\text{where } \mathbb{E}_{q_l}[(x_i^Tb_l)^2] = \sum_{j=1}^p \alpha_{jl} x_{ij}^2(\sigma_{jl}^2 + \mu_{jl}^2)
\end{aligned}
\]</span> Where <span class="math inline">\(\alpha_{jl}, \sigma_{jl}^2, \mu_{jl}\)</span> are the posterior inclusion probability for entry <span class="math inline">\(j\)</span> in <span class="math inline">\(b_l\)</span>, the posterior variance of that entry when selected, and the posterior mean of that entry when selected, selectively.</p>
<p>According to Wolfram Alpha, by taking the derivative of the objective w.r.t. <span class="math inline">\(\xi_i\)</span>, we get a derivative of <span class="math display">\[
\frac{(-2e^{\xi_i} \xi_i + e^{2\xi_i} - 1) \cdot (\xi_i^2 - Q_i)} {4(e^{\xi_i} + 1)^2 \xi_i^2} := 0 \iff \xi_i^2 = \pm Q_i \Rightarrow \xi_i := +\sqrt{Q_i}
\]</span> As an interpretation, <span class="math inline">\(\xi_i^2\)</span> is the second moment of the linear predictor <span class="math inline">\(x_i^T \mathbf{b}\)</span> under the current estimates for <span class="math inline">\(q_l\)</span>.</p>
<p>With these updates derived, we can now implement the algorithm.</p>
<div id="a-note-on-the-interceptcovariates" class="section level3">
<h3>A Note on the Intercept/Covariates</h3>
<p>In regular SuSiE, we can center our covariates and response to avoid fitting the intercept. Since our data is now 0/1, we can no longer center our response.</p>
<p>In <a href="http://stephenslab.uchicago.edu/assets/papers/Carbonetto2012.pdf">Carbonetto and Stephens (2012)</a> and <a href="https://arxiv.org/pdf/1709.06597.pdf">Carbonetto, Zhou, and Stephens (2017)</a>, Peter put a flat prior (normal with large variance) on the effects for the covariates <span class="math inline">\(Z\)</span> (including the intercept, if <span class="math inline">\(Z\)</span> has a column of 1’s). He then integrates out the effects from the likelihood.</p>
<p>Instead of taking this approach, I have simply included the effects (I refer to them as <span class="math inline">\(\delta\)</span>) on the covariates (<span class="math inline">\(Z\)</span>) in the optimization and treat them as nuisance parameters to be optimized over. I suspect these two methods aren’t to different due to the flat prior Peter uses. However, Peter’s method likely takes into account the variability of the estimates for <span class="math inline">\(\delta\)</span>.</p>
<p>In deriving the new updates, we follow the exact same steps as before, except we replace all instances of <span class="math inline">\(x_i^T \beta\)</span> with <span class="math inline">\(x_i^T \beta + z_i^T \delta\)</span>, and treat <span class="math inline">\(\delta\)</span> as a constant in all expectations. Skipping the derivations, the resulting updates are: <span class="math display">\[
\begin{aligned}
q(\gamma_l = j) \propto \pi_j \cdot \sqrt{\frac{1}{\tau_{jl}}} \cdot \exp \Bigg\{\frac{\nu_{jl}^2}{2\tau_{jl}}\Bigg\} \\
b_{l_j}|\gamma_l = j \sim_{q_l} \mathcal{N}\Big(\frac{\nu_{jl}}{\tau_{jl}}, \frac{1}{\tau_{jl}}\Big) \\
\text{where } \nu_{jl} = \sum_{i=1}^n x_{ij}\Big(y_i - \frac{1}{2} - \frac{1}{\xi_i}(g(\xi_i) - \frac{1}{2})(x_i^T \sum_{k \ne l}\bar{b_k} + z_i^T \delta)\Big) \text{ and } \tau_{jl} = \frac{1}{\sigma_0^2} + \sum_{i=1}^n \frac{1}{\xi_i}(g(\xi_i) - \frac{1}{2})x_{ij}^2 \\
\xi_i = +\sqrt{\mathbb{E}_{\beta \sim q}\Big[(x_i^T \sum_{l=1}^L b_l + z_i^T \delta)^2\Big]} = \sqrt{\sum_{j=1}^p \alpha_{jl} x_{ij}^2(\sigma_{jl}^2 + \mu_{jl}^2) + 2 \sum_{l=1}^L \sum_{k &gt; l}^L (x_i^T \bar{b_l})(x_i^T \bar{b_k}) + 2(x_i^T \bar{\beta})(z_i^T \delta) + (z_i^T \delta)^2} \\
\delta = (Z^TDZ)^{-1}Z^T(y - \frac{1}{2} - DX\bar{\beta}) \\
\text{where } D = diag\Big(\frac{1}{\xi_i}(g(\xi_i) - \frac{1}{2})\Big)
\end{aligned}
\]</span></p>
</div>
<div id="computing-the-elbo" class="section level3">
<h3>Computing the ELBO</h3>
<p>The derivation of calculating the ELBO using the lower-bound to the log-likelihood can be found below: <span class="math display">\[
\begin{aligned}
ELBO(q) = \mathbb{E}_q[\log (p(Y|\beta, X, Z, \delta))] - D_{KL}(q \| p) \ge \mathbb{E}_q[\log(h(\beta, \delta, \xi))] - D_{KL}(q \|p) =: \widetilde{ELBO(q)} \\
\text{For the first term, we get:} \\
\mathbb{E}_q[\log(h(\beta, \delta, \xi))] = \sum_{i=1}^n \log(g(\xi_i)) + (y_i - \frac{1}{2})(x_i^T \bar{\beta} + z_i^T \delta) - \frac{1}{2} \xi_i - \frac{1}{2 \xi_i}(g(\xi_i) - \frac{1}{2})(\mathbb{E}_q[(x_i^T \beta + z_i^T \delta)^2] - \xi_i^2) \\
\text{Since we optimize } \xi_i := \sqrt{\mathbb{E}_q[(x_i^T \beta + z_i^T \delta)^2]} \text{, the entire last term vanishes, and we&#39;re left with} \\
\mathbb{E}_q[\log(h(\beta, \delta, \xi))] = \sum_{i=1}^n \log(g(\xi_i)) + (y_i - \frac{1}{2})(x_i^T \bar{\beta} + z_i^T \delta) - \frac{1}{2} \xi_i \text{, after we optimize } \xi_i \\
\text{For the second term, we get:} \\
D_{KL}(q \|p) = \mathbb{E}_q \Big[\log\Big(\frac{q_1(b_1) \cdots q_L(b_L)}{p(b_1) \cdots p(b_L)}\Big)\Big] = \sum_{l=1}^L \mathbb{E}_{q_l}\Big[\log\Big(\frac{q_l(b_l)}{p(b_l)}\Big)\Big] \\
\text{Looking at each component individually, since they all take the same form:} \\
\mathbb{E}_{q_l}\Big[\log\Big(\frac{q_l(b_l)}{p(b_l)}\Big)\Big] = \int q(b_l) \cdot \log\Big(\frac{q(b_l)}{p(b_l)}\Big) db_l = \sum_{j=1}^p \alpha_{jl} \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma_{jl}^2}} e^{\frac{1}{2\sigma_{jl}^2} (c - \mu_{jl})^2} \log\Big(\frac{\alpha_{jl}\frac{1}{\sqrt{2\pi\sigma_{jl}^2}} e^{\frac{1}{2\sigma_{jl}^2} (c - \mu_{jl})^2}}{\pi_j \frac{1}{\sqrt{2\pi\sigma_0^2}} e^{\frac{1}{2\sigma_0^2} (c - 0)^2}}\Big) dc = \\
\sum_{j=1}^p \alpha_{jl} \Big[\log(\alpha_{jl}) - \log(\pi_j) + D_{KL}(\mathcal{N}(\mu_{jl}, \sigma_{jl}^2 \| \mathcal{N}(0, \sigma_0^2)\Big] = \\
\sum_{j=1}^p \alpha_{jl} \Big[\log(\alpha_{jl}) - \log(\pi_j) + \frac{1}{2} \log(\sigma_0^2) - \frac{1}{2} \log(\sigma_{jl}^2) - \frac{1}{2} + \frac{1}{2\sigma_0^2}(\sigma_{jl}^2 + \mu_{jl}^2)\Big] \Rightarrow \\
D_{KL}(q \|p) = \sum_{l=1}^L \sum_{j=1}^p \alpha_{jl} \Big[\log(\alpha_{jl}) - \log(\pi_j) + \frac{1}{2} \log(\sigma_0^2) - \frac{1}{2} \log(\sigma_{jl}^2) - \frac{1}{2} + \frac{1}{2\sigma_0^2}(\sigma_{jl}^2 + \mu_{jl}^2)\Big] \\
\therefore \widetilde{ELBO(q)} = \sum_{i=1}^n \log(g(\xi_i)) + (y_i - \frac{1}{2})(x_i^T \bar{\beta} + z_i^T \delta) - \frac{1}{2} \xi_i - \sum_{l=1}^L \sum_{j=1}^p \alpha_{jl} \Big[\log(\alpha_{jl}) - \log(\pi_j) + \frac{1}{2} \log(\sigma_0^2) - \frac{1}{2} \log(\sigma_{jl}^2) - \frac{1}{2} + \frac{1}{2\sigma_0^2}(\sigma_{jl}^2 + \mu_{jl}^2)\Big]
\end{aligned}
\]</span></p>
<p>It is worth noting that we could conceivably optimize this objective function directly, either by adding the appropriate constraints on <span class="math inline">\(\alpha, \sigma\)</span>, or by making a transformation of variables to avoid constraints. We could then derive the gradient of the objective function and optimize this objective with standard optimization algorithms, e.g. BFGS. However, I have not done so.</p>
</div>
<div id="estimating-sigma_02" class="section level3">
<h3>Estimating <span class="math inline">\(\sigma_0^2\)</span></h3>
<p>I have also derived a method for estimating <span class="math inline">\(\sigma_0^2\)</span>, the prior variance of the non-zero effects. I do this by optimizing the ELBO with respect to <span class="math inline">\(\sigma_0^2\)</span> in each iteration.</p>
<p>By taking the partial derivative of the ELBO with respect to <span class="math inline">\(\sigma_0^2\)</span> and setting it to 0, we find that the ELBO is maximizezd when <span class="math display">\[
\sigma_0^2 = \frac{\sum_{l=1}^L \sum_{j=1}^p \alpha_{jl}(\sigma_{jl}^2 + \mu_{jl}^2)}{\sum_{l=1}^L \sum_{j=1}^p \alpha_{jl}} = \frac{\sum_{l=1}^L \sum_{j=1}^p \alpha_{jl}(\sigma_{jl}^2 + \mu_{jl}^2)}{L}
\]</span></p>
<p>We can also take the second derivative, and we can confirm that the ELBO is concave at this point.</p>
</div>
</div>
<div id="algebraic-comparison-between-vb-and-glm-algorithms" class="section level2">
<h2>Algebraic Comparison between VB and GLM Algorithms</h2>
<p>In that follows, I will refer to the new VB approach as the <strong>VB Algorithm</strong>, and the original iterative GLM approach as the <strong>GLM Algorithm</strong>.</p>
<p>The form of the updates in both algorithms are very similar. This is not surprising, since in both cases the posterior distribution takes the same form: a multinoulli draw for the non-zero element of <span class="math inline">\(b_l\)</span>, and then a normal draw for the corresponding effect size.</p>
<p>In the SER step of the GLM algorithm, we obtain MLE estimates for <span class="math inline">\(\hat{b_j}\)</span> and its SE <span class="math inline">\(\sigma_{MLE_j}\)</span>. We then compute the posterior mean and variance of the normal distribution: <span class="math display">\[
\begin{aligned}
b|\gamma_j = 1 \sim \mathcal{N}(\mu_{1j}, \sigma_{1j}^2) \\
\mu_{1j} = \frac{\sigma_{1j}^2}{\sigma_{MLE_j}^2}\hat{b_j} \\
\sigma_{1j}^2 = \frac{1}{1/\sigma_0^2 + 1/\sigma_{MLE_j}^2}
\end{aligned}
\]</span></p>
<p>In the updates from the VB algorithm, we get <span class="math display">\[
\begin{aligned}
b|\gamma_j = 1 \sim \mathcal{N}(\mu_{1j}, \sigma_{1j}^2) \\
\mu_{1j} = \frac{\sum_{i=1}^n x_{ij}\Big(y_i - \frac{1}{2} - \frac{1}{\xi_i}(g(\xi_i) - \frac{1}{2})(x_i^T \sum_{k \ne l}\bar{b_k})\Big)}{1/\sigma_0^2 + \sum_{i=1}^n \frac{1}{\xi_i}(g(\xi_i) - \frac12)(x_{ij}^2)} \\
\sigma_{1j}^2 = \frac{1}{1/\sigma_0^2 + \sum_{i=1}^n \frac{1}{\xi_i}(g(\xi_i) - \frac12)(x_{ij}^2)} \\
\end{aligned}
\]</span></p>
<p>In order to connect these two update rules, recall what we actually do in the GLM case for updating <span class="math inline">\(b_l\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>We run a logistic regression of <span class="math inline">\(Y \sim X_j\)</span> keeping <span class="math inline">\(b_{-l}\)</span> fixed at its posterior mean, <span class="math inline">\(\bar{b_{-l}}\)</span>. This gives us <span class="math inline">\(\widehat{b_{lj, MLE}}\)</span> and <span class="math inline">\(\sigma_{lj, MLE}^2\)</span>;</p></li>
<li><p>We then find the posterior distribution using the asymptotic normality of the MLE.</p></li>
</ol>
<p>Let’s see what happens if we apply this same method, but instead of using the true logistic likelihood and find the MLE, instead use our variational lower-bound on the likelihood <span class="math inline">\(h(b_l, \bar{b_{-l}}, \xi)\)</span>. Since we are essentially regression on just one <span class="math inline">\(X_j\)</span> in this step, <span class="math inline">\(b_l\)</span> will take the form <span class="math inline">\(c \cdot e_j\)</span> for basis vector <span class="math inline">\(e_j\)</span> and effect size <span class="math inline">\(c\)</span> which has a prior distribution of <span class="math inline">\(\mathcal{N}(0, \sigma_0^2)\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
\log(h(ce_j, \bar{b_{-l}}, \xi)) = \sum_{i=1}^n \log(g(\xi_i)) + y_ix_i^T\bar{b_{-l}} + y_icx_{ij} - \frac{x_i^T\bar{b_{-l}} + cx_{ij} + \xi_i}{2} - \frac{1}{2\xi_i}((g(\xi_i) - \frac12)((x_i^T\bar{b_{-l}} + cx_{ij})^2 - \xi_i^2) \Rightarrow \\
\frac{\partial}{\partial c} \log(h(ce_j, \bar{b_{-l}}, \xi)) = \sum_{i=1}^n y_i x_{ij} - \frac{x_{ij}}{2} - \frac{1}{2\xi_i}(g(\xi_i) - \frac12)2x_{ij}(x_i^T\bar{b_{-l}} + cx_{ij}) := 0 \Rightarrow \\
\hat{c}_{MLE&#39;} = \frac{\sum_{i=1}^n x_{ij}\Big[y_i - \frac12 - \frac{1}{\xi_i}(g(\xi_i) - \frac12)x_i^T\bar{b_{-l}}\Big]}{\sum_{i=1}^n x_{ij}^2 \frac{1}{\xi_i}(g(\xi_i) - \frac12)}
\end{aligned}
\]</span> Furthermore, if this were actually the likelihood, we could look at the second derivative to tell us about the variance: <span class="math display">\[
\frac{\partial^2}{\partial c^2} \log(h(ce_j, \bar{b_{-l}}, \xi)) = \sum_{i=1}^n-\frac{1}{\xi_i}(g(\xi_i) - \frac12)x_{ij}^2 \Rightarrow \sigma_{MLE&#39;}^2 = \frac{1}{\sum_{i=1}^n\frac{1}{\xi_i}(g(\xi_i) - \frac12)x_{ij}^2}
\]</span></p>
<p>If we proceed in the same manner as in the GLM updates, we would get: <span class="math display">\[
\begin{aligned}
\mu_{1j} = \frac{\sigma_{1j}^2}{\sigma_{MLE&#39;}^2}\hat{c}_{MLE&#39;} = \frac{\sum_{i=1}^n\frac{1}{\xi_i}(g(\xi_i) - \frac12)x_{ij}^2}{1 / \sigma_0^2 + \sum_{i=1}^n\frac{1}{\xi_i}(g(\xi_i) - \frac12)x_{ij}^2} \cdot \frac{\sum_{i=1}^n x_{ij}\Big[y_i - \frac12 - \frac{1}{\xi_i}(g(\xi_i) - \frac12)x_i^T\bar{b_{-l}}\Big]}{\sum_{i=1}^n x_{ij}^2 \frac{1}{\xi_i}(g(\xi_i) - \frac12)} = \frac{\sum_{i=1}^n x_{ij}\Big(y_i - \frac{1}{2} - \frac{1}{\xi_i}(g(\xi_i) - \frac{1}{2})(x_i^T \sum_{k \ne l}\bar{b_k})\Big)}{1/\sigma_0^2 + \sum_{i=1}^n \frac{1}{\xi_i}(g(\xi_i) - \frac12)(x_{ij}^2)} \\
\sigma_{1j}^2 = \frac{1}{1/\sigma_0^2 + 1/\sigma_{MLE_j}^2} = \frac{1}{1 / \sigma_0^2 + \sum_{i=1}^n\frac{1}{\xi_i}(g(\xi_i) - \frac12)x_{ij}^2}
\end{aligned}
\]</span></p>
<p>Note that these are exactly the same updates we get from the VB method.</p>
<p>As for calculating the PIPs, in the GLM algorithm, we have <span class="math display">\[
\alpha_j \propto \pi_j \frac{\mathcal{N}(\hat{b_j}; 0, \sigma_0^2 + \sigma_{MLE_j}^2)}{\mathcal{N}(\hat{b_j}; 0, \sigma_{MLE_j}^2 )} = \pi_j \sqrt{\frac{\sigma_{MLE_j}^2}{\sigma_0^2 + \sigma_{MLE_j}^2}} \exp\Bigg\{\frac{\hat{b_j}^2}{2}\Big(\frac{1}{\sigma_{MLE_j}^2} - \frac{1}{\sigma_0^2 + \sigma_{MLE_j}^2}\Big)\Bigg\}
\]</span></p>
<p>In the VB algorithm, we have <span class="math display">\[
\alpha_j \propto \pi_j \sqrt{\sigma_{1j}^2} \exp\Bigg\{\frac{\mu_{1j}^2}{2\sigma_{1j}^2}\Bigg\}
\]</span></p>
<p>Now, if we start from the PIPs from the GLM version and plug in our results when applied to the VB algorithm: <span class="math display">\[
\begin{aligned}
\alpha_j \propto \pi_j \sqrt{\frac{\sigma_{MLE_j}^2}{\sigma_0^2 + \sigma_{MLE_j}^2}} \exp\Bigg\{\frac{\hat{c_j}^2}{2}\Big(\frac{1}{\sigma_{MLE_j}^2} - \frac{1}{\sigma_0^2 + \sigma_{MLE_j}^2}\Big)\Bigg\} = \pi_j \sqrt{\frac{1}{\sigma_0^2/\sigma_{MLE_j}^2 + 1}} \exp\Bigg\{\frac{\hat{c}_j^2}{2}\Big(\frac{\sigma_0^2}{(\sigma_{MLE_j}^2)(\sigma_0^2 + \sigma_{MLE_j}^2)}\Big)\Bigg\} = \\
[\hat{c}_j^2 = \mu_{1j}^2 \frac{\sigma_{MLE_j}^4}{\sigma_{1j}^4}] = \pi_j \sqrt{\frac{1}{\sigma_0^2(1/\sigma_{MLE_j}^2 + 1 / \sigma_0^2)}}\exp\Bigg\{\mu_{1j}^2 \frac{\sigma_{MLE_j}^4}{2\sigma_{1j}^4}\Big(\frac{\sigma_0^2}{(\sigma_{MLE_j}^2)(\sigma_0^2 + \sigma_{MLE_j}^2)}\Big)\Bigg\} \propto \\
[\sqrt{1 / \sigma_0^2} \; \text{constant}, \sigma_{1j}^2 = \frac{1}{1/\sigma_0^2 + 1/\sigma_{MLE_j}^2}]  \propto \pi_j \sqrt{\sigma_{1j}^2} \exp\Bigg\{\frac{\mu_{1j}^2}{2\sigma_{1j}^2} \cdot \Big(\frac{\sigma_0^2 \sigma_{MLE_j}^2}{\sigma_{1j}^2(\sigma_0^2 + \sigma_{MLE_j}^2)}\Big)\Bigg\} = \\
[\sigma_{1j}^2 = \frac{1}{1/\sigma_0^2 + 1/\sigma_{MLE_j}^2}] = \pi_j \sqrt{\sigma_{1j}^2} \exp\Bigg\{\frac{\mu_{1j}^2}{2\sigma_{1j}^2} \cdot \Big(\frac{\sigma_0^2 \sigma_{MLE_j}^2(1/\sigma_0^2 + 1/\sigma_{MLE_j}^2)}{\sigma_0^2 + \sigma_{MLE_j}^2}\Big)\Bigg\} = \\
\pi_j \sqrt{\sigma_{1j}^2} \exp\Bigg\{\frac{\mu_{1j}^2}{2\sigma_{1j}^2} \cdot \Big(\frac{\sigma_0^2 + \sigma_{MLE_j}^2}{\sigma_0^2 + \sigma_{MLE_j}^2}\Big)\Bigg\} = \pi_j \sqrt{\sigma_{1j}^2} \exp\Bigg\{\frac{\mu_{1j}^2}{2\sigma_{1j}^2}\Bigg\}
\end{aligned}
\]</span> This is the same form we get directly from the VB update.</p>
<p>This seems like magic at first! But when we think about it, the VB lower-bound we use makes the pseudo-likelihood <span class="math inline">\(h\)</span> a quadratic, so of course we should recover the original SuSiE rules (which we co-opted for use in the GLM algorithm).</p>
<p>So naturally, one might ask what the problem is with the GLM version. Since it uses the same general method and uses the true likelihood function, shouldn’t it be better?</p>
<p>The answer lies in our fixing the other parameters to their posterior mean, <span class="math inline">\(\bar{b}_{-l}\)</span>. In the update to <span class="math inline">\(q_l\)</span>, we set <span class="math display">\[
q_l(c \cdot e_j) \propto \mathbb{E}_{-(q_l)}[\log(p(y|b_l = c \cdot e_j, b_{-l}) p(b_l = c \cdot e_j)) = \mathbb{E}_{-(q_l)}\Big[\log(\pi_j) + \log\Big(\mathcal{N}(c; 0, \sigma_0^2)\Big) + \sum_{i=1}^n y_ix_i^T(b_l + \sum_{k \ne l} b_k) - \log\Big(1 + \exp\{x_i^T(b_l + \sum_{k \ne l} b_k\}\Big)\Big]
\]</span> The issue with the GLM approach is that by finding the MLE for <span class="math inline">\(b_l\)</span> by treating <span class="math inline">\(b_{-l}\)</span> fixed at its posterior mean under <span class="math inline">\(q_{-l}\)</span> is that we are essentially saying that <span class="math inline">\(\mathbb{E}_{-(q_l)}\Big[\log\Big(1 + \exp\{x_i^T(b_l + \sum_{k \ne l} b_k\}\Big)\Big] = \log\Big(1 + \exp\{x_i^T(b_l + \bar{b}_{-l}\}\Big)\)</span>. If it were the case that <span class="math inline">\(\mathbb{E}_{-(q_l)}\Big[\log\Big(1 + \exp\{x_i^T(b_l + \sum_{k \ne l} b_k\}\Big)\Big] \le \log\Big(1 + \exp\{x_i^T(b_l + \bar{b}_{-l}\}\Big)\)</span>, then we would have no issue, since we would be maintaining the chain of “<span class="math inline">\(\ge\)</span>” used in the derivation of VB (subtracting a smaller value vs. a larger value in the likelihood). However, <span class="math inline">\(\log(1 + e^x)\)</span> is a concave function, so by Jensen’s inequality, <span class="math inline">\(\mathbb{E}_{-(q_l)}\Big[\log\Big(1 + \exp\{x_i^T(b_l + \sum_{k \ne l} b_k\}\Big)\Big] \ge \log\Big(1 + \exp\{x_i^T(b_l + \bar{b}_{-l}\}\Big)\)</span> (the opposite of what we want).</p>
<p>We would have approximate equality in the above case if <span class="math inline">\(\log(1 + e^x)\)</span> were roughly linear. The second derivative of <span class="math inline">\(\log(1 + e^x)\)</span> is <span class="math inline">\(\frac{e^x}{(1 + e^x)^2}\)</span>. This second derivative has a maximum of <span class="math inline">\(\frac14\)</span> at <span class="math inline">\(x = 0\)</span>, and looks like this:</p>
<pre class="r"><code>curve(exp(x) / ((1 + exp(x))^2), -5, 5, xlab = &quot;x&quot;, ylab = &quot;Second Derivative&quot;)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-1-1">
Past versions of unnamed-chunk-1-1.png
</button>
</p>
<div id="fig-unnamed-chunk-1-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/68bcd880028c037b3359b879ded235ee9c6ff6b5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-1-1.png" target="_blank">68bcd88</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-1-1.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>For larger values of <span class="math inline">\(|x|\)</span>, the second derivative is quite small, and thus <span class="math inline">\(\log(1+e^x)\)</span> is roughly linear, so taking the expectation insude the function doesn’t change things too much. And even at the maximum of 0, the second derivative is still on the smaller side. So one could argue that our implicit approximation in the GLM algorithm works well, even if the inequality is technically in the wrong direction.</p>
<p>The benefit of the VB approach is that we retain some information about the higher moments of <span class="math inline">\(b_{-l}\)</span> in the form of our <span class="math inline">\(\xi_i\)</span> variational parameters.</p>
<p>With this in mind, I suspect that if differences in the method can be found, it will be when the linear predictors <span class="math inline">\(x_i^T\mathbf{b}\)</span> are all close to 0.</p>
</div>
</div>
<div id="implementation" class="section level1">
<h1>Implementation</h1>
<p>To see the functions, please see “/code/logistic_susie_VB_functions.R” in the github folder.</p>
<pre class="r"><code>source(&quot;./code/logistic_susie_VB_functions.R&quot;) # read in VB code
source(&quot;./code/logistic_susie_glm_functions.R&quot;) # read in old GLM version</code></pre>
</div>
<div id="demonstration" class="section level1">
<h1>Demonstration</h1>
<div id="easy-demonstration" class="section level2">
<h2>Easy Demonstration</h2>
<div id="l-1" class="section level3">
<h3>L = 1</h3>
<p>As a very easy demonstration, let <span class="math inline">\(n = 100, p = 10, L = 1\)</span>, and let all columns of <span class="math inline">\(X\)</span> be distributed <span class="math inline">\(\mathcal{N}(0, 1)\)</span>. WLOG, we let the first element of <span class="math inline">\(\mathbf{b}\)</span> be the only non-zero element. We set <span class="math inline">\(\sigma_0^2 = 1\)</span>.</p>
<p>We then simulated <span class="math inline">\(\mathbf{y}\)</span> from the specified bernoulli model, <span class="math display">\[
y_i \stackrel{\perp}{\sim} \text{Bern}\Bigg(\frac{e^{(\mathbf{Xb})_i}}{1 + e^{(\mathbf{Xb})_i}}\Bigg)
\]</span></p>
<p>We repeat this procedure 100 times. We do fit an intercept here.</p>
<pre class="r"><code>set.seed(1138)

n = 100
p = 10
L = 1
V = 1
beta_true = rep(0, p)

B = 100
b_1s = numeric(B)
susie.fits.easy = list() # logistic SuSiE fits
logistic.fits.easy = list() # logistic regression fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  # make response
  Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
  susie.fits.easy[[i]] = susie_logistic_VB(Y, X, L, V)
  logistic.fits.easy[[i]] = glm(Y ~ X, family = &quot;binomial&quot;)
}</code></pre>
<p>Figure 1 below plots the estimated value for <span class="math inline">\(\hat{b}_1 := \mu_1 \cdot \alpha_1\)</span> against the true value, where <span class="math inline">\(\mu_1\)</span> is the estimated posterior mean, and <span class="math inline">\(\alpha_1\)</span> is the estimated posterior inclusion probability. Figure 2 below plots the PIP, <span class="math inline">\(\alpha_i\)</span>, against the true value of <span class="math inline">\(b_1\)</span>.</p>
<pre class="r"><code>plot(sapply(susie.fits.easy, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Estimate&quot;, main = &quot;Figure 1&quot;, pch = 19)
abline(0, 1)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-4-1">
Past versions of unnamed-chunk-4-1.png
</button>
</p>
<div id="fig-unnamed-chunk-4-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-4-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-4-1.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/5c41e85c0cb729173b651c9b0751c2a567e12ae3/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-4-1.png" target="_blank">5c41e85</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-4-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/c62537b6e3875acc75330707685f6af94ddf16a2/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-4-1.png" target="_blank">c62537b</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(sapply(susie.fits.easy, function(x) x$post_alpha[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;Figure 2&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-4-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-4-2">
Past versions of unnamed-chunk-4-2.png
</button>
</p>
<div id="fig-unnamed-chunk-4-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-4-2.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-4-2.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/5c41e85c0cb729173b651c9b0751c2a567e12ae3/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-4-2.png" target="_blank">5c41e85</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-4-2.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Figure 1 shows that, in this simulation, we observe a shrinkage behavior similar to hard-thresholding (with a small thresholding value). We also see that our estimated lie pretty close to the truth. Figure 2 shows a sharp transition between a very low PIP and a PIP near 1. This sharp transition is what causes the hard-thresholding-like shrinkage we obsersve in figure 1: the posterior mean values <span class="math inline">\(\mu_1\)</span> closely match the true values for <span class="math inline">\(b_1\)</span>, but are effectively shrunk to 0 in this region by the PIP <span class="math inline">\(\alpha_i\)</span>, and are effectively untouched outside this region.</p>
<p>We can compare figure 2 with the p-values obtained from the normal logistic regressions on the same simulated data, shown in figure 3 below.</p>
<pre class="r"><code>plot(sapply(logistic.fits.easy, function(x) summary(x)$coefficients[2, 4]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Logistic p-value&quot;, main = &quot;Figure 3&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-5-1">
Past versions of unnamed-chunk-5-1.png
</button>
</p>
<div id="fig-unnamed-chunk-5-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-5-1.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-5-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We see a similar phase transition around the same values of the true effect size, <span class="math inline">\(|b_1| \le 0.5\)</span>.</p>
</div>
<div id="l-2" class="section level3">
<h3>L = 2</h3>
<p>We now repeat the same as above, except with <span class="math inline">\(L = 2, \sigma_0^2 = 1\)</span>, and the second element <span class="math inline">\(b_2\)</span> also a non-zero.</p>
<pre class="r"><code>set.seed(1138)

n = 100
p = 10
L = 2
V = 1
beta_true = rep(0, p)

B = 100
b_1s = numeric(B)
b_2s = numeric(B)
susie.fits.easy2 = list() # logistic SuSiE fits
logistic.fits.easy2 = list() # logistic regression fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  beta_true[2] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  b_2s[i] = beta_true[2]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  # make response
  Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
  susie.fits.easy2[[i]] = susie_logistic_VB(Y, X, L, V, maxit = 1000)
  logistic.fits.easy2[[i]] = glm(Y ~ X, family = &quot;binomial&quot;)
}</code></pre>
<p>Figures 4(a-b) below plot the estimated value for <span class="math inline">\(\hat{b}_1 := \sum_{l = 1}^2 \mu_{l,1} \cdot \alpha_{l,1}\)</span> and <span class="math inline">\(\hat{b}_2 := \sum_{l = 1}^2 \mu_{l,2} \cdot \alpha_{l,2}\)</span> against the true value, where <span class="math inline">\(\mu_{l,i}\)</span> is the estimated posterior mean, and <span class="math inline">\(\alpha_{l,i}\)</span> is the estimated posterior inclusion probability, for entry <span class="math inline">\(i\)</span> in estimated vector <span class="math inline">\(l\)</span>. Figures 5(a-b) below plot the maximum PIP, <span class="math inline">\(\max_{l \in \{1, 2\}}\alpha_{l,i}\)</span>, against the true value of <span class="math inline">\(b_i\)</span>.</p>
<pre class="r"><code>#est_b1_max = sapply(susie.fits.easy2, function(x) x$post_alpha[1, which.max(x$post_alpha[1, ]) ]* x$post_mu[1, which.max(x$post_alpha[1, ])])

#est_b2_max = sapply(susie.fits.easy2, function(x) x$post_alpha[2, which.max(x$post_alpha[2, ]) ]* x$post_mu[2, which.max(x$post_alpha[2, ])])

est_alpha1_max = sapply(susie.fits.easy2, function(x) max(x$post_alpha[1, ]))

est_alpha2_max = sapply(susie.fits.easy2, function(x) max(x$post_alpha[2, ]))

est_b = sapply(susie.fits.easy2, function(x) rowSums(x$post_alpha * x$post_mu))

plot(est_b[1, ] ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Estimate&quot;, main = &quot;Figure 4(a) - Entry 1&quot;, pch = 19)
abline(0, 1)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-7-1">
Past versions of unnamed-chunk-7-1.png
</button>
</p>
<div id="fig-unnamed-chunk-7-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-1.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/5c41e85c0cb729173b651c9b0751c2a567e12ae3/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-1.png" target="_blank">5c41e85</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(est_b[2, ] ~ b_2s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Estimate&quot;, main = &quot;Figure 4(b) - Entry 2&quot;, pch = 19)
abline(0, 1)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-7-2">
Past versions of unnamed-chunk-7-2.png
</button>
</p>
<div id="fig-unnamed-chunk-7-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-2.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-2.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/5c41e85c0cb729173b651c9b0751c2a567e12ae3/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-2.png" target="_blank">5c41e85</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-2.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(est_alpha1_max ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;Figure 5(a) - Entry 1&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-3.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-7-3">
Past versions of unnamed-chunk-7-3.png
</button>
</p>
<div id="fig-unnamed-chunk-7-3" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-3.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-3.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/5c41e85c0cb729173b651c9b0751c2a567e12ae3/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-3.png" target="_blank">5c41e85</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-3.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(est_alpha2_max ~ b_2s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;Figure 5(b) - Entry 2&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-4.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-7-4">
Past versions of unnamed-chunk-7-4.png
</button>
</p>
<div id="fig-unnamed-chunk-7-4" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-4.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-4.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/5c41e85c0cb729173b651c9b0751c2a567e12ae3/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-4.png" target="_blank">5c41e85</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-7-4.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We see a similar pattern as above, in the case <span class="math inline">\(L = 1\)</span>. The method is also able to detect both effects (when strong enough).</p>
<p>We can compare figures 5(a-b) with the p-values obtained from the normal logistic regressions on the same simulated data, shown in figures 6(a-b) below.</p>
<pre class="r"><code>plot(sapply(logistic.fits.easy2, function(x) summary(x)$coefficients[2, 4]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Logistic p-value&quot;, main = &quot;Figure 6(a) - Entry 1&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-8-1">
Past versions of unnamed-chunk-8-1.png
</button>
</p>
<div id="fig-unnamed-chunk-8-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-8-1.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-8-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/acba6fcd7a27dc3fadefb8369f0d96276ae4a67c/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-8-1.png" target="_blank">acba6fc</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(sapply(logistic.fits.easy2, function(x) summary(x)$coefficients[3, 4]) ~ b_2s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Logistic p-value&quot;, main = &quot;Figure 6(b) - Entry 2&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-8-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-8-2">
Past versions of unnamed-chunk-8-2.png
</button>
</p>
<div id="fig-unnamed-chunk-8-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-8-2.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-8-2.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/acba6fcd7a27dc3fadefb8369f0d96276ae4a67c/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-8-2.png" target="_blank">acba6fc</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We see a similar phase transition around the same values of the true effect size, <span class="math inline">\(|b_i| \le 0.5\)</span>.</p>
</div>
</div>
<div id="medium-demonstration" class="section level2">
<h2>Medium Demonstration</h2>
<p>As a medium difficulty demonstration, let <span class="math inline">\(n = 100, p = 10, L = 3, \sigma_0^2 = 5\)</span>.</p>
<p>We create the data <span class="math inline">\(\mathbf{X}\)</span> where all entries are iid standard normal. I then set the 2nd column to be identical to the 3rd, and the 7th column identical to the 6th (when running this simulation, <span class="math inline">\(b_3\)</span> and <span class="math inline">\(b_6\)</span> were generated to be non-zero, and <span class="math inline">\(b_2\)</span> and <span class="math inline">\(b_7\)</span> were generated to be 0, so this was done in the spirit of the toy example from the paper).</p>
<p>We then simulated <span class="math inline">\(\mathbf{y}\)</span> from the specified bernoulli model, <span class="math display">\[
y_i \stackrel{\perp}{\sim} \text{Bern}\Bigg(\frac{e^{(\mathbf{Xb})_i}}{1 + e^{(\mathbf{Xb})_i}}\Bigg)
\]</span></p>
<pre class="r"><code>### TEST susie_logistic_VB
set.seed(1138)

n = 100
p = 10
L = 3
pi = rep(1 / p, p) # prior weights
V = 5 # prior variance

beta_true = rep(0, p)
for (l in 1:L) {
  b_l = rnorm(1, 0, sqrt(V))
  gamma_l = rmultinom(1, 1, pi)
  beta_l = b_l * gamma_l
  beta_true = beta_true + beta_l
}

# simulate data, induce correlations
X = matrix(rnorm(n*p, 0, 1), nrow = n, ncol = p)
X[, 2] = X[, 3]
X[, 7] = X[, 6]

# make response
Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))

susie.logistic.fit = susie_logistic_VB(Y, X, L, V)</code></pre>
<p>Figure 11 below plots the true values for <span class="math inline">\(\mathbf{b}\)</span>.</p>
<pre class="r"><code>plot(beta_true, ylab = &quot;True Value&quot;, main = &quot;Figure 11&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-10-1">
Past versions of unnamed-chunk-10-1.png
</button>
</p>
<div id="fig-unnamed-chunk-10-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-10-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The 3rd and 6th are around -1.3, and the 5th is around 0.75.</p>
<p>Figures 12(a-c) below plots the posterior means for the 3 vectors estimated from this procedure (calculated as <span class="math inline">\(\mathbf{\bar{b}}_l := \alpha_l \circ \mu_{1l}\)</span> using <span class="math inline">\(\alpha_l, \mu_{1l}\)</span> returned from the logistic-version of the IBFS algorithm):</p>
<pre class="r"><code>b_1_post = susie.logistic.fit$post_mu[, 1] * susie.logistic.fit$post_alpha[, 1]
b_2_post = susie.logistic.fit$post_mu[, 2] * susie.logistic.fit$post_alpha[, 2]
b_3_post = susie.logistic.fit$post_mu[, 3] * susie.logistic.fit$post_alpha[, 3]

plot(b_1_post, ylab = &quot;Posterior Mean Value&quot;, main = &quot;Figure 12(a)&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-11-1">
Past versions of unnamed-chunk-11-1.png
</button>
</p>
<div id="fig-unnamed-chunk-11-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-1.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-1.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/acba6fcd7a27dc3fadefb8369f0d96276ae4a67c/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-1.png" target="_blank">acba6fc</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(b_2_post, ylab = &quot;Posterior Mean Value&quot;, main = &quot;Figure 12(b)&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-11-2">
Past versions of unnamed-chunk-11-2.png
</button>
</p>
<div id="fig-unnamed-chunk-11-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-2.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-2.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-2.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(b_3_post, ylab = &quot;Posterior Mean Value&quot;, main = &quot;Figure 12(c)&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-3.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-11-3">
Past versions of unnamed-chunk-11-3.png
</button>
</p>
<div id="fig-unnamed-chunk-11-3" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-3.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-3.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-11-3.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We can see that the first estimated vector captures our constructed correlation between the 2nd and 3rd columns, the second estimated vector captures our constructed correlation between the 6th and 7th columns, and the third estimated vector captures the individual effect of the 5th column.</p>
<p>Figures 13(a-c) below plots our estimated PIPs, <span class="math inline">\(\alpha_1, \alpha_2, \alpha_3\)</span>:</p>
<pre class="r"><code>plot(susie.logistic.fit$post_alpha[, 1], ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;Figure 13(a)&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-12-1">
Past versions of unnamed-chunk-12-1.png
</button>
</p>
<div id="fig-unnamed-chunk-12-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-1.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-1.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/acba6fcd7a27dc3fadefb8369f0d96276ae4a67c/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-1.png" target="_blank">acba6fc</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/c62537b6e3875acc75330707685f6af94ddf16a2/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-1.png" target="_blank">c62537b</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(susie.logistic.fit$post_alpha[, 2], ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;Figure 13(b)&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-12-2">
Past versions of unnamed-chunk-12-2.png
</button>
</p>
<div id="fig-unnamed-chunk-12-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-2.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-2.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-2.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(susie.logistic.fit$post_alpha[, 3], ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;Figure 13(c)&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-3.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-12-3">
Past versions of unnamed-chunk-12-3.png
</button>
</p>
<div id="fig-unnamed-chunk-12-3" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-3.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-3.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/5c41e85c0cb729173b651c9b0751c2a567e12ae3/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-3.png" target="_blank">5c41e85</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-12-3.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>These groups of correlated predictors are shows from the PIPs.</p>
</div>
<div id="hard-demonstration" class="section level2">
<h2>Hard Demonstration</h2>
<p>As a hard demonstration, let <span class="math inline">\(n = 1,000, p = 100, L = 10, \sigma_0^2 = 5\)</span>.</p>
<p>We create the data <span class="math inline">\(\mathbf{X}\)</span> where all entries are iid standard normal. I then set the 2nd column to be identical to the 1st, the 88th column identical to the 87th, and the 89th column highly negatively correlated with the 87th (when running this simulation, <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_87\)</span> were generated to be non-zero, and <span class="math inline">\(b_2\)</span>, <span class="math inline">\(b_{88}\)</span> and <span class="math inline">\(b_{89}\)</span> were generated to be 0, so this was done in the spirit of the toy example from the paper).</p>
<p>We then simulated <span class="math inline">\(\mathbf{y}\)</span> from the specified bernoulli model, <span class="math display">\[
y_i \stackrel{\perp}{\sim} \text{Bern}\Bigg(\frac{e^{(\mathbf{Xb})_i}}{1 + e^{(\mathbf{Xb})_i}}\Bigg)
\]</span></p>
<pre class="r"><code>set.seed(1138)

n = 1000
p = 100
L = 10
pi = rep(1 / p, p) # prior weights
V = 5 # prior variance

beta_true = rep(0, p)
for (l in 1:L) {
  b_l = rnorm(1, 0, sqrt(V))
  gamma_l = rmultinom(1, 1, pi)
  beta_l = b_l * gamma_l
  beta_true = beta_true + beta_l
}

# simulate data, induce correlations
X = matrix(rnorm(n*p, 0, 1), nrow = n, ncol = p)
X[, 2] = X[, 1]
X[, 88] = X[, 87]
X[, 89] = runif(n, -1, -.7) * X[, 87]

# make response
Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))

susie.logistic.fit = susie_logistic_VB(Y, X, L, V)</code></pre>
<p>Figure 14 below plots the true values for <span class="math inline">\(\mathbf{b}\)</span> (the numbers correspond to the points and indices of non-zero true effects).</p>
<pre class="r"><code>plot(beta_true[beta_true == 0] ~ which(beta_true == 0), xlim = c(0, 101), ylim = range(beta_true) + c(-.1, .1), xlab = &quot;Index&quot;, ylab = &quot;True Value&quot;, main = &quot;Figure 14&quot;, pch = 19)
text(which(beta_true != 0), beta_true[beta_true != 0], labels = which(beta_true != 0))</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-14-1">
Past versions of unnamed-chunk-14-1.png
</button>
</p>
<div id="fig-unnamed-chunk-14-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-14-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We can see that true effects 41 and 56 are very small, and 27, 60, and 87 are also small-ish.</p>
<p>Figures 15(a-j) below plots the posterior means for the 10 vectors estimated from this procedure (calculated as <span class="math inline">\(\mathbf{\bar{b}}_l := \alpha_l \circ \mu_{1l}\)</span> using <span class="math inline">\(\alpha_l, \mu_{1l}\)</span> returned from the logistic-version of the IBFS algorithm):</p>
<pre class="r"><code>par(mfrow = c(5, 2))
for (l in 1:L) {
  b_l_post = susie.logistic.fit$post_mu[, l] * susie.logistic.fit$post_alpha[, l]
  plot(b_l_post[abs(b_l_post) &lt; .01] ~ which(abs(b_l_post) &lt; .01), xlim = c(0, 101), ylim = range(b_l_post) + c(-.1, .1), xlab = &quot;Index&quot;, ylab = &quot;Posterior Mean Value&quot;, main = paste(&quot;Figure 15(&quot;, letters[l], &quot;)&quot;, sep = &quot;&quot;), pch = 19)
  text(which(abs(b_l_post) &gt;= .01), b_l_post[abs(b_l_post) &gt;= .01], labels = which(abs(b_l_post) &gt;= .01))
}</code></pre>
<pre><code>Error in text.default(which(abs(b_l_post) &gt;= 0.01), b_l_post[abs(b_l_post) &gt;= : zero-length &#39;labels&#39; specified</code></pre>
<pre class="r"><code>par(mfrow = c(1, 1))</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-15-1">
Past versions of unnamed-chunk-15-1.png
</button>
</p>
<div id="fig-unnamed-chunk-15-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-15-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-15-1.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-15-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>(NOTE: In figures 15(a) and 15(h), 1 and 2 are close together, so it looks like the number 12. But it is really just 1 and 2).</p>
<p>We can see that the first estimated vector (15a) (and 8th, 15(h)) captures our constructed correlation between the 1st and 2nd columns, and 8th vector (15h) captures our constructed correlation between the 87th, 88th, and 89th columns (note the signs: 89 was constructed to be negatively correlated with the true effect column 87).</p>
<p>Figures 16(a-j) below plots our estimated PIPs, <span class="math inline">\(\alpha_1, \alpha_2, \alpha_3\)</span>:</p>
<pre class="r"><code>par(mfrow = c(5, 2))
for (l in 1:L) {
  alpha_l = susie.logistic.fit$post_alpha[, l]
  plot(alpha_l[alpha_l &lt; .01] ~ which(alpha_l &lt; .01), xlim = c(0, 101), ylim = c(0, 1), xlab = &quot;Index&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = paste(&quot;Figure 16(&quot;, letters[l], &quot;)&quot;, sep = &quot;&quot;), pch = 19)
  text(which(alpha_l &gt;= .01), alpha_l[alpha_l &gt;= .01], labels = which(alpha_l &gt;= .01))
}</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-16-1">
Past versions of unnamed-chunk-16-1.png
</button>
</p>
<div id="fig-unnamed-chunk-16-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-16-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-16-1.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-16-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/c62537b6e3875acc75330707685f6af94ddf16a2/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-16-1.png" target="_blank">c62537b</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>par(mfrow = c(1, 1))</code></pre>
<p>These groups of correlated predictors are shows from the PIPs.</p>
</div>
<div id="misspecified-l" class="section level2">
<h2>Misspecified <span class="math inline">\(L\)</span></h2>
<p>In regular SuSiE, the algorithm is robust to misspecifying the value for <span class="math inline">\(L\)</span>. For instance, say there are 3 true groups of correlated predictors. Then even we specify, say, <span class="math inline">\(L = 5\)</span>, the algorithm will give us 3 meaningful credible sets and 7 impure ones (see sections 3.4.2 and 3.5 of the SuSiE paper). In this section, I show via example that this appears to hold true in the logistic version as well.</p>
<p>For example, consider the easy case where <span class="math inline">\(n = 1000\)</span>, <span class="math inline">\(p = 10\)</span>, each entry <span class="math inline">\(X_{ij} \stackrel{iid}{\sim} \mathcal{N}(0, 1)\)</span>, and the first three effects are <span class="math inline">\(b_1 = 1, b_2 = -0.75, b_3 = 0.5\)</span>. I test when I set <span class="math inline">\(L = 3\)</span> (the optimal value), <span class="math inline">\(L = 5\)</span>, and <span class="math inline">\(L = 10\)</span>.</p>
<pre class="r"><code>set.seed(1138)

n = 1000
p = 10
L = 1
beta_true = rep(0, p)
beta_true[1:3] = c(1, -.75, .5)
V = 1

X = matrix(rnorm(n * p), nrow = n)
# make response
Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))

fit.susie.log3 = susie_logistic_VB(Y, X, L = 3, V, maxit = 1000)
fit.susie.log5 = susie_logistic_VB(Y, X, L = 5, V, maxit = 1000)
fit.susie.log10 = susie_logistic_VB(Y, X, L = 10, V, maxit = 1000)

par(mfcol = c(3, 2))
matplot(fit.susie.log3$post_alpha, xlab = &quot;Index&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;L = 3&quot;)
matplot(fit.susie.log5$post_alpha, xlab = &quot;Index&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;L = 5&quot;)
matplot(fit.susie.log10$post_alpha, xlab = &quot;Index&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;L = 10&quot;)

plot(rowSums(fit.susie.log3$post_alpha * fit.susie.log3$post_mu), xlab = &quot;Index&quot;, ylab = &quot;Posterior Mean&quot;, main = &quot;L = 3&quot;, pch = 19)
plot(rowSums(fit.susie.log5$post_alpha * fit.susie.log5$post_mu), xlab = &quot;Index&quot;, ylab = &quot;Posterior Mean&quot;, main = &quot;L = 5&quot;, pch = 19)
plot(rowSums(fit.susie.log10$post_alpha * fit.susie.log10$post_mu), xlab = &quot;Index&quot;, ylab = &quot;Posterior Mean&quot;, main = &quot;L = 10&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-17-1">
Past versions of unnamed-chunk-17-1.png
</button>
</p>
<div id="fig-unnamed-chunk-17-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-17-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-17-1.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-17-1.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/5c41e85c0cb729173b651c9b0751c2a567e12ae3/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-17-1.png" target="_blank">5c41e85</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-17-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/acba6fcd7a27dc3fadefb8369f0d96276ae4a67c/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-17-1.png" target="_blank">acba6fc</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/c62537b6e3875acc75330707685f6af94ddf16a2/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-17-1.png" target="_blank">c62537b</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>par(mfcol = c(1, 1))</code></pre>
<p>We can see that having a value of <span class="math inline">\(L\)</span> too large adds groups with a lot of impurity in the PIPs (these groups being roughly identical, which is peculiar to me). Unsurprisingly, since our PIPs are forced to sum to 1, adding more groups will always add to the posterior means of each coefficient (assuming effects from different groups don’t cancel out due to having opposite signs). We can see this on the right side of the plots above (for <span class="math inline">\(L = 3\)</span>, coefficients 4-10 are close to 0, but for <span class="math inline">\(L = 5\)</span> and <span class="math inline">\(L = 10\)</span>, these effects start to deviate from 0).</p>
</div>
<div id="misspecified-sigma_02" class="section level2">
<h2>Misspecified <span class="math inline">\(\Sigma_0^2\)</span></h2>
<p>In this section, I test how robust the algorithm is to misspecifying the prior variance, <span class="math inline">\(\sigma_0^2\)</span>, supplied to the algorithm.</p>
</div>
<div id="comparison-between-the-vb-algorithm-and-the-original-iterative-glm-modification-of-regular-susie" class="section level2">
<h2>Comparison Between the VB Algorithm and the Original Iterative GLM Modification of Regular SuSiE</h2>
<p>In that follows, I will refer to the new VB approach as the <strong>VB Algorithm</strong>, and the original iterative GLM approach as the <strong>GLM Algorithm</strong>.</p>
<p><strong>NOTE: I have so far been unable to find many meaningful differences between the two algorithms</strong></p>
<div id="convergence" class="section level3">
<h3>Convergence</h3>
<p>First, before diving into any simulation examples, recall that for the GLM algorithm, I encountered situations where the algorithm didn’t converge (e.g. cycling behavior). To remedy this, I modified the convergence criteria to also take into account the estimated means from 2 iterations prior (not just the previous iteration). In the VB algorithm, since it is a hill climbing algorithm, we should not encounter scenarios where the algorithm fails to converge.</p>
</div>
<div id="easy-comparison" class="section level3">
<h3>Easy Comparison</h3>
<p>Here, I use the same simulations as from the “Easy Demonstration” section above. We have <span class="math inline">\(n = 100\)</span>, <span class="math inline">\(p = 10\)</span>, <span class="math inline">\(L = 1\)</span>, <span class="math inline">\(\sigma_0 = 1\)</span>, and set the first coefficient <span class="math inline">\(b_1\)</span> to be non-zero.</p>
<pre class="r"><code>set.seed(1138)

n = 100
p = 10
L = 1
V = 1
beta_true = rep(0, p)

B = 100
b_1s = numeric(B)
susie.fits.easy = list() # GLM logistic SuSiE fits
susie.fits.easy.VB = list() # VB logistic SuSiE fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  # make response
  Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
  susie.fits.easy[[i]] = susie_logistic(Y, X, L, V, intercept = F) # fit without intercept to compare directly w/ VB method
  susie.fits.easy.VB[[i]] = susie_logistic_VB(Y, X, L, V)
}

par(mfcol = c(3, 2))
plot(sapply(susie.fits.easy, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Estimate&quot;, main = &quot;GLM SuSiE Coefficient Estimates: n = 100&quot;, pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy, function(x) x$post_alpha[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;GLM SuSiE PIP Estimates: n = 100&quot;, pch = 19, ylim = c(0, 1))

plot(sapply(susie.fits.easy, function(x) x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Mu Estimate&quot;, main = &quot;GLM SuSiE Coefficient Estimates: n = 100&quot;, pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy.VB, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Estimate&quot;, main = &quot;VB SuSiE Coefficient Estimates: n = 100&quot;, pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy.VB, function(x) x$post_alpha[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;VB SuSiE PIP Estimates: n = 100&quot;, pch = 19, ylim = c(0, 1))

plot(sapply(susie.fits.easy.VB, function(x) x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Mu Estimate&quot;, main = &quot;VB SuSiE Coefficient Estimates: n = 100&quot;, pch = 19)
abline(0, 1)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-18-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-18-1">
Past versions of unnamed-chunk-18-1.png
</button>
</p>
<div id="fig-unnamed-chunk-18-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-18-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-18-1.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-18-1.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/5c41e85c0cb729173b651c9b0751c2a567e12ae3/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-18-1.png" target="_blank">5c41e85</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-18-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/acba6fcd7a27dc3fadefb8369f0d96276ae4a67c/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-18-1.png" target="_blank">acba6fc</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/c62537b6e3875acc75330707685f6af94ddf16a2/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-18-1.png" target="_blank">c62537b</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>par(mfcol = c(1, 1))</code></pre>
<p>Qualitatively, these two methods appear nearly identical in this example. The only difference I see is in the PIPs for true effects around -1 to -1.25. The GLM method has two PIPs around 0.9, whereas almost all PIPs for the VB method are around 1.</p>
<p>To make things slightly harder, I perform the experiment again, except I not set <span class="math inline">\(n = 50\)</span> (so the effect should be harder to identify).</p>
<pre class="r"><code>set.seed(1138)

n = 50
p = 10
L = 1
V = 1
beta_true = rep(0, p)

B = 100
b_1s = numeric(B)
susie.fits.easy = list() # GLM logistic SuSiE fits
susie.fits.easy.VB = list() # VB logistic SuSiE fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  # make response
  Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
  susie.fits.easy[[i]] = susie_logistic(Y, X, L, V, intercept = F) # fit without intercept to compare directly w/ VB method
  susie.fits.easy.VB[[i]] = susie_logistic_VB(Y, X, L, V)
}

par(mfcol = c(3, 2))
plot(sapply(susie.fits.easy, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Estimate&quot;, main = &quot;GLM SuSiE Coefficient Estimates: n = 50&quot;, pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy, function(x) x$post_alpha[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;GLM SuSiE PIP Estimates: n = 50&quot;, pch = 19, ylim = c(0, 1))

plot(sapply(susie.fits.easy, function(x) x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Mu Estimate&quot;, main = &quot;GLM SuSiE Coefficient Estimates: n = 50&quot;, pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy.VB, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Estimate&quot;, main = &quot;VB SuSiE Coefficient Estimates: n = 50&quot;, pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy.VB, function(x) x$post_alpha[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;VB SuSiE PIP Estimates: n = 50&quot;, pch = 19, ylim = c(0, 1))

plot(sapply(susie.fits.easy.VB, function(x) x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Mu Estimate&quot;, main = &quot;VB SuSiE Coefficient Estimates: n = 50&quot;, pch = 19)
abline(0, 1)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-19-1">
Past versions of unnamed-chunk-19-1.png
</button>
</p>
<div id="fig-unnamed-chunk-19-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-19-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/40ae4d48747742ebc5d09188526857043bba8e40/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-19-1.png" target="_blank">40ae4d4</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-19-1.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/7785c2e32567d29f4ed4a44619d343110fa75fa5/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-19-1.png" target="_blank">7785c2e</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/5c41e85c0cb729173b651c9b0751c2a567e12ae3/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-19-1.png" target="_blank">5c41e85</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-19-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>par(mfcol = c(1, 1))</code></pre>
<p>The differences here are more apparent. First, the GLM coefficient estimates for large true effects are biased towards 0 far more than the VB estimates. The driving factor is likely the PIPs, where the GLM version has a hard time; even for large effects, the PIPs are still only at aroung 0.9 at best. In contrast, the VB method returns PIPs of 1 for large enough effects.</p>
<p>Back in the setting where <span class="math inline">\(n = 100\)</span>, let’s see what happends when we increase the prior variance and aim for larger effect sizes:</p>
<pre class="r"><code>set.seed(1138)

n = 100
p = 10
L = 1
V = 25
beta_true = rep(0, p)

B = 1000
b_1s = numeric(B)
susie.fits.easy = list() # GLM logistic SuSiE fits
susie.fits.easy.VB = list() # VB logistic SuSiE fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  # make response
  Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
  susie.fits.easy[[i]] = susie_logistic(Y, X, L, V, intercept = F) # fit without intercept to compare directly w/ VB method
  susie.fits.easy.VB[[i]] = susie_logistic_VB(Y, X, L, V)
}</code></pre>
<pre><code>Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre><code>Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre class="r"><code>par(mfcol = c(3, 2))
plot(sapply(susie.fits.easy, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Estimate&quot;, main = &quot;GLM SuSiE Coefficient Estimates: n = 100&quot;, pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy, function(x) x$post_alpha[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;GLM SuSiE PIP Estimates: n = 100&quot;, pch = 19, ylim = c(0, 1))

plot(sapply(susie.fits.easy, function(x) x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Mu Estimate&quot;, main = &quot;GLM SuSiE Coefficient Estimates: n = 100&quot;, pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy.VB, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Estimate&quot;, main = &quot;VB SuSiE Coefficient Estimates: n = 100&quot;, pch = 19)
abline(0, 1)

plot(sapply(susie.fits.easy.VB, function(x) x$post_alpha[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;VB SuSiE PIP Estimates: n = 100&quot;, pch = 19, ylim = c(0, 1))

plot(sapply(susie.fits.easy.VB, function(x) x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Mu Estimate&quot;, main = &quot;VB SuSiE Coefficient Estimates: n = 100&quot;, pch = 19)
abline(0, 1)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-20-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-20-1">
Past versions of unnamed-chunk-20-1.png
</button>
</p>
<div id="fig-unnamed-chunk-20-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-20-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/40ae4d48747742ebc5d09188526857043bba8e40/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-20-1.png" target="_blank">40ae4d4</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/6fbb126884cd980a6cecec0d2fa929595ce840eb/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-20-1.png" target="_blank">6fbb126</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-08
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/9fa2301ad69c033faef97129b5abb99a185eb653/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-20-1.png" target="_blank">9fa2301</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-07
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/c62537b6e3875acc75330707685f6af94ddf16a2/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-20-1.png" target="_blank">c62537b</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>par(mfcol = c(1, 1))</code></pre>
<p>Wow! Things can really go wrong with the GLM algorithm, it seems. To see what’s going on, let’s look at the point where the posterior estimate is almost 0, but the true effect is almost -10.</p>
<pre class="r"><code>i_bad = which(sapply(susie.fits.easy, function(x) x$post_alpha[1]) &lt; .45 &amp; b_1s&lt; -9)

set.seed(1138)

for (i in 1:(i_bad-1)) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  # make response
  Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
  #susie.fits.easy[[i]] = susie_logistic(Y, X, L, V, intercept = F) # fit without intercept to compare directly w/ VB method
  #susie.fits.easy.VB[[i]] = susie_logistic_VB(Y, X, L, V)
}
beta_true[1] = rnorm(1, 0, sqrt(V))
b_1s[i] = beta_true[1]
# make independent N(0, 1) covariates
X = matrix(rnorm(n * p), nrow = n)
# make response
Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))
susie.fits = susie_logistic(Y, X, L, V, intercept = F) # fit without intercept to compare directly w/ VB method</code></pre>
<pre><code>Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred

Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<pre class="r"><code>susie.fits.easy.VB = susie_logistic_VB(Y, X, L, V)</code></pre>
<p>Upon further inspection, it appears that this is happening due to the data being separable, so the GLM algorithm does not converge and has huge standard errors for the estimates. In this case, I am curious as to what the VB algorithm is doing, since it appears to be working fine. I believe in this case it is relying heavily on the prior variance I specify when calling the algorithm. This is confirmed when I change the prior variance I tell the algorithm to use, which leads to very different estimated effect sizes.</p>
</div>
<div id="hard-comparison" class="section level3">
<h3>Hard Comparison</h3>
<p>In this setting, I set <span class="math inline">\(n = 100\)</span>, <span class="math inline">\(p = 10\)</span>, and <span class="math inline">\(\sigma_0 = 1\)</span>. I generate each observation of <span class="math inline">\(X\)</span> as coming from a multivariate normal distribution with mean <span class="math inline">\(0\)</span> and covariance structured such that all diagonal entries are 1, and for groups of correlated entries we get 0.95. The correlated groups are (1, 2, 3), (4, 5), and (9, 10). I then set the following entries in <span class="math inline">\(\mathbf{b}\)</span> to be non-zero: 1, 4, 6, 7 (two are in correlated groups, two are not, and there is a correlated group on zero effect variables).</p>
<pre class="r"><code>set.seed(1138)
n = 100
p = 10
V = 1
L = 4

Sigma = diag(.05, nrow = 10)
Sigma[1:3, 1:3] = Sigma[1:3, 1:3] + matrix(.95, nrow = 3, ncol = 3) # 1, 2, 3 correlated 
Sigma[4:5, 4:5] = Sigma[4:5, 4:5] + matrix(.95, nrow = 2, ncol = 2) # 4, 5 correlated
Sigma[6:8, 6:8] = Sigma[6:8, 6:8] + diag(.95, nrow = 3)
Sigma[9:10, 9:10] = Sigma[9:10, 9:10] + matrix(.95, nrow = 2, ncol = 2) # 9, 10 correlated

X = MASS::mvrnorm(n, rep(0, p), Sigma)
beta_true = rep(0, p)
beta_true[c(1, 4, 6, 7)] = rnorm(4, 0, sqrt(V))
Y = rbinom(n, 1, exp(X %*% beta_true) / (1 + exp(X %*% beta_true)))

ptm = proc.time()
susie.fit.glm = susie_logistic(Y, X, L, V, intercept = F)
ptm.glm = proc.time() - ptm
ptm = proc.time()
susie.fit.vb = susie_logistic_VB(Y, X, L, V)
ptm.vb =proc.time() - ptm

plot(beta_true[beta_true == 0] ~ which(beta_true == 0), xlim = c(0, 11), ylim = range(beta_true) + c(-.1, .1), xlab = &quot;Index&quot;, ylab = &quot;True Value&quot;, main = &quot;True Effects&quot;, pch = 19)
text(which(beta_true != 0), beta_true[beta_true != 0], labels = which(beta_true != 0))</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-22-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-22-1">
Past versions of unnamed-chunk-22-1.png
</button>
</p>
<div id="fig-unnamed-chunk-22-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-22-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>par(mfrow = c(2, 2))
for (l in 1:L) {
  b_l_post = susie.fit.glm$post_mu[, l] * susie.fit.glm$post_alpha[, l]
  plot(b_l_post[abs(b_l_post) &lt; .01] ~ which(abs(b_l_post) &lt; .01), xlim = c(0, 11), ylim = range(b_l_post) + c(-.1, .1), xlab = &quot;Index&quot;, ylab = &quot;Posterior Mean Value&quot;, main = paste(&quot;GLM SuSiE Posterior Mean No. &quot;, l, sep = &quot;&quot;), pch = 19)
  text(which(abs(b_l_post) &gt;= .01), b_l_post[abs(b_l_post) &gt;= .01], labels = which(abs(b_l_post) &gt;= .01))
}</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-22-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-22-2">
Past versions of unnamed-chunk-22-2.png
</button>
</p>
<div id="fig-unnamed-chunk-22-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-22-2.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>par(mfrow = c(1, 1))

par(mfrow = c(2, 2))
for (l in 1:L) {
  b_l_post = susie.fit.vb$post_mu[, l] * susie.fit.vb$post_alpha[, l]
  plot(b_l_post[abs(b_l_post) &lt; .01] ~ which(abs(b_l_post) &lt; .01), xlim = c(0, 11), ylim = range(b_l_post) + c(-.1, .1), xlab = &quot;Index&quot;, ylab = &quot;Posterior Mean Value&quot;, main = paste(&quot;VB SuSiE Posterior Mean No. &quot;, l, sep = &quot;&quot;), pch = 19)
  text(which(abs(b_l_post) &gt;= .01), b_l_post[abs(b_l_post) &gt;= .01], labels = which(abs(b_l_post) &gt;= .01))
}</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-22-3.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-22-3">
Past versions of unnamed-chunk-22-3.png
</button>
</p>
<div id="fig-unnamed-chunk-22-3" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-22-3.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>par(mfrow = c(1, 1))</code></pre>
<p>From these plots, we see that the posterior mean estimates in both algorithms are sensible and nearly indistinguishable.</p>
</div>
<div id="computation-time-comparison" class="section level3">
<h3>Computation Time Comparison</h3>
<p>In the previous example, the GLM algorithm took 0.87 seconds to converge and the VB algorithm took 0.07 seconds to converge (both with the same convergence tolerance). The VB method appears to be much faster. This is not surprising, since the GLM algorithm makes many calls the R’s built-in <code>glm</code> function, which requires us to run an optimization algorithm. For the VB method, since we are essentially finding an analytical solution of an approximation to the same problem, the algorithm runs much faster.</p>
<p><strong>HOWEVER, I have found that in most cases, their runtimes are virtually the same</strong></p>
</div>
</div>
<div id="including-covariates" class="section level2">
<h2>Including Covariates</h2>
<p>In this section, I add a matrix of covariates, <span class="math inline">\(Z\)</span>. The data generating procedure is now <span class="math inline">\(y_i \stackrel{\perp}{\sim} \text{Bern}\Bigg(\frac{e^{(\mathbf{Xb} + \mathbf{Z\delta})_i}}{1 + e^{(\mathbf{Xb} + \mathbf{Z\delta})_i}}\Bigg)\)</span></p>
<div id="random-covariates" class="section level3">
<h3>Random Covariates</h3>
<p>Here, I generate the covariates <span class="math inline">\(Z\)</span> independently from <span class="math inline">\(X\)</span>.</p>
<p>I revert back to the same scenario as the first demonstration (<span class="math inline">\(n = 100, p = 10, L = 1, \sigma_0^2 = 1\)</span>), except now I add a random matrix of 1 covariate, generate random effects, and also a random intercept.</p>
<pre class="r"><code>set.seed(1138)

n = 100
p = 10
q = 1
L = 1
V = 1
beta_true = rep(0, p)

B = 100
b_1s = numeric(B)
susie.fits.easy = list() # logistic SuSiE fits
logistic.fits.easy = list() # logistic regression fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  Z = matrix(rnorm(n * q), nrow = n)
  delta = rnorm(q, 0, sqrt(V))
  int = rnorm(1)
  # make response
  Y = rbinom(n, 1, exp((X %*% beta_true) + (Z %*% delta) + int) / (1 + exp((X %*% beta_true) + (Z %*% delta) + int)))
  susie.fits.easy[[i]] = susie_logistic_VB(Y, X, L, V, Z = Z)
  logistic.fits.easy[[i]] = glm(Y ~ X + Z, family = &quot;binomial&quot;)
}</code></pre>
<p>The first figure below plots the estimated value for <span class="math inline">\(\hat{b}_1 := \mu_1 \cdot \alpha_1\)</span> against the true value, where <span class="math inline">\(\mu_1\)</span> is the estimated posterior mean, and <span class="math inline">\(\alpha_1\)</span> is the estimated posterior inclusion probability. The second figure below plots the PIP, <span class="math inline">\(\alpha_i\)</span>, against the true value of <span class="math inline">\(b_1\)</span>.</p>
<pre class="r"><code>plot(sapply(susie.fits.easy, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Estimate&quot;, main = &quot;Estimated Effects when Including Covariates and Intercept&quot;, pch = 19)
abline(0, 1)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-24-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-24-1">
Past versions of unnamed-chunk-24-1.png
</button>
</p>
<div id="fig-unnamed-chunk-24-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-24-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/acba6fcd7a27dc3fadefb8369f0d96276ae4a67c/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-24-1.png" target="_blank">acba6fc</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(sapply(susie.fits.easy, function(x) x$post_alpha[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;Estimated PIPs when Including Covariates and Intercept&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-24-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-24-2">
Past versions of unnamed-chunk-24-2.png
</button>
</p>
<div id="fig-unnamed-chunk-24-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-24-2.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/acba6fcd7a27dc3fadefb8369f0d96276ae4a67c/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-24-2.png" target="_blank">acba6fc</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The first plot shows that, in this simulation, we observe a shrinkage behavior similar to hard-thresholding (with a small thresholding value). We also see that our estimates lie pretty close to the truth. The second shows a sharp transition between a very low PIP and a PIP near 1. This sharp transition is what causes the hard-thresholding-like shrinkage we obsersve in figure 1: the posterior mean values <span class="math inline">\(\mu_1\)</span> closely match the true values for <span class="math inline">\(b_1\)</span>, but are effectively shrunk to 0 in this region by the PIP <span class="math inline">\(\alpha_i\)</span>, and are effectively untouched outside this region.</p>
<p>We can compare these figures with the estimated effects and p-values obtained from the normal logistic regressions on the same simulated data, shown below.</p>
<pre class="r"><code>plot(sapply(logistic.fits.easy, function(x) summary(x)$coefficients[2, 1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Logistic Effect&quot;, main = &quot;Regular Logistic Regression Effects when Including Covariates and Intercept&quot;, pch = 19)
abline(0, 1)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-25-1">
Past versions of unnamed-chunk-25-1.png
</button>
</p>
<div id="fig-unnamed-chunk-25-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-25-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/acba6fcd7a27dc3fadefb8369f0d96276ae4a67c/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-25-1.png" target="_blank">acba6fc</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-05-06
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(sapply(logistic.fits.easy, function(x) summary(x)$coefficients[2, 4]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Logistic p-value&quot;, main = &quot;Regular Logistic Regression p-values when Including Covariates and Intercept&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-25-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-25-2">
Past versions of unnamed-chunk-25-2.png
</button>
</p>
<div id="fig-unnamed-chunk-25-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-25-2.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We see a similar phase transition around the same values of the true effect size, <span class="math inline">\(|b_1| \le 0.5\)</span>.</p>
</div>
<div id="covariates-correlated-with-predictors" class="section level3">
<h3>Covariates Correlated with Predictors</h3>
<p>Here, I generate a single covariate that has a correlation of .9 with our effect variable, and has an independent effect.</p>
<pre class="r"><code>set.seed(1138)

n = 100
p = 10
L = 1
V = 1
beta_true = rep(0, p)

B = 100
b_1s = numeric(B)
susie.fits.easy = list() # logistic SuSiE fits
logistic.fits.easy = list() # logistic regression fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  XZ = MASS::mvrnorm(n, rep(0, 2), matrix(c(1, .9, .9, 1), nrow = 2))
  X[, 1] = XZ[, 1]
  Z = as.matrix(XZ[, 2], ncol = 1)
  delta = rnorm(1, 0, sqrt(V))
  int = rnorm(1)
  # make response
  Y = rbinom(n, 1, exp((X %*% beta_true) + (Z %*% delta) + int) / (1 + exp((X %*% beta_true) + (Z %*% delta) + int)))
  susie.fits.easy[[i]] = susie_logistic_VB(Y, X, L, V, Z = Z)
  logistic.fits.easy[[i]] = glm(Y ~ X + Z, family = &quot;binomial&quot;)
}</code></pre>
<p>The first figure below plots the estimated value for <span class="math inline">\(\hat{b}_1 := \mu_1 \cdot \alpha_1\)</span> against the true value, where <span class="math inline">\(\mu_1\)</span> is the estimated posterior mean, and <span class="math inline">\(\alpha_1\)</span> is the estimated posterior inclusion probability. The second figure below plots the PIP, <span class="math inline">\(\alpha_i\)</span>, against the true value of <span class="math inline">\(b_1\)</span>.</p>
<pre class="r"><code>plot(sapply(susie.fits.easy, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Estimate&quot;, main = &quot;Estimated Effects when Including Covariates and Intercept&quot;, pch = 19)
abline(0, 1)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-27-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-27-1">
Past versions of unnamed-chunk-27-1.png
</button>
</p>
<div id="fig-unnamed-chunk-27-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-27-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(sapply(susie.fits.easy, function(x) x$post_alpha[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;Estimated PIPs when Including Covariates and Intercept&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-27-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-27-2">
Past versions of unnamed-chunk-27-2.png
</button>
</p>
<div id="fig-unnamed-chunk-27-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-27-2.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>It is obvious that the covariate is taking the entire effect away from the predictor. This makes sense in the context of the model, since we do not penalize the covariate effects in any way.</p>
<p>We can compare these figures with the estimated effects and p-values obtained from the normal logistic regressions on the same simulated data, shown below.</p>
<pre class="r"><code>plot(sapply(logistic.fits.easy, function(x) summary(x)$coefficients[2, 1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Logistic Effect&quot;, main = &quot;Regular Logistic Regression Effects when Including Covariates and Intercept&quot;, pch = 19)
abline(0, 1)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-28-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-28-1">
Past versions of unnamed-chunk-28-1.png
</button>
</p>
<div id="fig-unnamed-chunk-28-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-28-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(sapply(logistic.fits.easy, function(x) summary(x)$coefficients[2, 4]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Logistic p-value&quot;, main = &quot;Regular Logistic Regression p-values when Including Covariates and Intercept&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-28-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-28-2">
Past versions of unnamed-chunk-28-2.png
</button>
</p>
<div id="fig-unnamed-chunk-28-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-28-2.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>In this case, the regular logistic regression does a much better job, since it treats the covariate and predictors as the same.</p>
<p>Performing the same silumation, except with covariate that has no effect:</p>
<pre class="r"><code>set.seed(1138)

n = 100
p = 10
L = 1
V = 1
beta_true = rep(0, p)

B = 100
b_1s = numeric(B)
susie.fits.easy = list() # logistic SuSiE fits
logistic.fits.easy = list() # logistic regression fits
for (i in 1:B) {
  beta_true[1] = rnorm(1, 0, sqrt(V))
  b_1s[i] = beta_true[1]
  # make independent N(0, 1) covariates
  X = matrix(rnorm(n * p), nrow = n)
  XZ = MASS::mvrnorm(n, rep(0, 2), matrix(c(1, .9, .9, 1), nrow = 2))
  X[, 1] = XZ[, 1]
  Z = as.matrix(XZ[, 2], ncol = 1)
  delta = 0
  int = rnorm(1)
  # make response
  Y = rbinom(n, 1, exp((X %*% beta_true) + (Z %*% delta) + int) / (1 + exp((X %*% beta_true) + (Z %*% delta) + int)))
  susie.fits.easy[[i]] = susie_logistic_VB(Y, X, L, V, Z = Z)
  logistic.fits.easy[[i]] = glm(Y ~ X + Z, family = &quot;binomial&quot;)
}</code></pre>
<p>The first figure below plots the estimated value for <span class="math inline">\(\hat{b}_1 := \mu_1 \cdot \alpha_1\)</span> against the true value, where <span class="math inline">\(\mu_1\)</span> is the estimated posterior mean, and <span class="math inline">\(\alpha_1\)</span> is the estimated posterior inclusion probability. The second figure below plots the PIP, <span class="math inline">\(\alpha_i\)</span>, against the true value of <span class="math inline">\(b_1\)</span>.</p>
<pre class="r"><code>plot(sapply(susie.fits.easy, function(x) x$post_alpha[1] * x$post_mu[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Estimate&quot;, main = &quot;Estimated Effects when Including Covariates and Intercept&quot;, pch = 19)
abline(0, 1)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-30-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-30-1">
Past versions of unnamed-chunk-30-1.png
</button>
</p>
<div id="fig-unnamed-chunk-30-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-30-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(sapply(susie.fits.easy, function(x) x$post_alpha[1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Posterior Inclusion Probability&quot;, main = &quot;Estimated PIPs when Including Covariates and Intercept&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-30-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-30-2">
Past versions of unnamed-chunk-30-2.png
</button>
</p>
<div id="fig-unnamed-chunk-30-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-30-2.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We still see the same behavior as above.</p>
<p>We can compare these figures with the estimated effects and p-values obtained from the normal logistic regressions on the same simulated data, shown below.</p>
<pre class="r"><code>plot(sapply(logistic.fits.easy, function(x) summary(x)$coefficients[2, 1]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Logistic Effect&quot;, main = &quot;Regular Logistic Regression Effects when Including Covariates and Intercept&quot;, pch = 19)
abline(0, 1)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-31-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-31-1">
Past versions of unnamed-chunk-31-1.png
</button>
</p>
<div id="fig-unnamed-chunk-31-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-31-1.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>plot(sapply(logistic.fits.easy, function(x) summary(x)$coefficients[2, 4]) ~ b_1s, xlab = &quot;True Effect Value&quot;, ylab = &quot;Logistic p-value&quot;, main = &quot;Regular Logistic Regression p-values when Including Covariates and Intercept&quot;, pch = 19)</code></pre>
<p><img src="figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-31-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-31-2">
Past versions of unnamed-chunk-31-2.png
</button>
</p>
<div id="fig-unnamed-chunk-31-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/andrewg3311/susieR_logistic_wflow/blob/f4dfb8a5aaed6f46a1d86064092aef7740ee7f3a/docs/figure/VB_susie_logistic_demonstration.Rmd/unnamed-chunk-31-2.png" target="_blank">f4dfb8a</a>
</td>
<td>
Andrew Goldstein
</td>
<td>
2019-07-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>In this case, the regular logistic regression does a much better job, since it treats the covariate and predictors as the same.</p>
</div>
</div>
<div id="estimating-sigma_02-1" class="section level2">
<h2>Estimating <span class="math inline">\(\sigma_0^2\)</span></h2>
<p>This section tests the estimation of the prior variance. <em>I’LL ADD THIS IN A BIT</em></p>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span> Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.5.1 (2018-07-02)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 17763)

Matrix products: default

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
 [1] workflowr_1.4.0 Rcpp_1.0.1      lattice_0.20-35 digest_0.6.19  
 [5] rprojroot_1.3-2 MASS_7.3-50     grid_3.5.1      backports_1.1.4
 [9] git2r_0.25.2    magrittr_1.5    evaluate_0.14   stringi_1.4.3  
[13] fs_1.3.1        whisker_0.3-2   Matrix_1.2-17   rmarkdown_1.13 
[17] tools_3.5.1     stringr_1.4.0   glue_1.3.1      xfun_0.7       
[21] yaml_2.2.0      compiler_3.5.1  htmltools_0.3.6 knitr_1.23     </code></pre>
</div>
</div>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
